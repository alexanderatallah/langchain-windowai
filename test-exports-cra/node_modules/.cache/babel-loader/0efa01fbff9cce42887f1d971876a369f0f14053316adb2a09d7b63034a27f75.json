{"ast":null,"code":"import { InMemoryCache } from \"../cache.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\n/**\n * LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n  constructor(_ref) {\n    let {\n      cache,\n      concurrency,\n      ...rest\n    } = _ref;\n    super(concurrency ? {\n      maxConcurrency: concurrency,\n      ...rest\n    } : rest);\n    /**\n     * The name of the LLM class\n     */\n    Object.defineProperty(this, \"name\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"cache\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    if (typeof cache === \"object\") {\n      this.cache = cache;\n    } else if (cache) {\n      this.cache = InMemoryCache.global();\n    } else {\n      this.cache = undefined;\n    }\n  }\n  async generatePrompt(promptValues, stop) {\n    const prompts = promptValues.map(promptValue => promptValue.toString());\n    return this.generate(prompts, stop);\n  }\n  /** @ignore */\n  async _generateUncached(prompts, stop) {\n    await this.callbackManager.handleLLMStart({\n      name: this._llmType()\n    }, prompts, this.verbose);\n    let output;\n    try {\n      output = await this._generate(prompts, stop);\n    } catch (err) {\n      await this.callbackManager.handleLLMError(err, this.verbose);\n      throw err;\n    }\n    await this.callbackManager.handleLLMEnd(output, this.verbose);\n    return output;\n  }\n  /**\n   * Run the LLM on the given propmts an input, handling caching.\n   */\n  async generate(prompts, stop) {\n    if (!Array.isArray(prompts)) {\n      throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n    }\n    if (!this.cache) {\n      return this._generateUncached(prompts, stop);\n    }\n    const {\n      cache\n    } = this;\n    const params = this.serialize();\n    params.stop = stop;\n    const llmStringKey = `${Object.entries(params).sort()}`;\n    const missingPromptIndices = [];\n    const generations = await Promise.all(prompts.map(async (prompt, index) => {\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (!result) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(missingPromptIndices.map(i => prompts[i]), stop);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        return cache.update(prompts[promptIndex], llmStringKey, generation);\n      }));\n      llmOutput = results.llmOutput ?? {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n   */\n  async call(prompt, stop) {\n    const {\n      generations\n    } = await this.generate([prompt], stop);\n    return generations[0][0].text;\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  _modelType() {\n    return \"base_llm\";\n  }\n  /**\n   * Load an LLM from a json-like object describing it.\n   */\n  static async deserialize(data) {\n    const {\n      _type,\n      _model,\n      ...rest\n    } = data;\n    if (_model && _model !== \"base_llm\") {\n      throw new Error(`Cannot load LLM with model ${_model}`);\n    }\n    const Cls = {\n      openai: (await import(\"./openai.js\")).OpenAI\n    }[_type];\n    if (Cls === undefined) {\n      throw new Error(`Cannot load  LLM with type ${_type}`);\n    }\n    return new Cls(rest);\n  }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n  async _generate(prompts, stop) {\n    const generations = [];\n    for (let i = 0; i < prompts.length; i += 1) {\n      const text = await this._call(prompts[i], stop);\n      generations.push([{\n        text\n      }]);\n    }\n    return {\n      generations\n    };\n  }\n}","map":{"version":3,"names":["InMemoryCache","BaseLanguageModel","BaseLLM","constructor","_ref","cache","concurrency","rest","maxConcurrency","Object","defineProperty","enumerable","configurable","writable","value","global","undefined","generatePrompt","promptValues","stop","prompts","map","promptValue","toString","generate","_generateUncached","callbackManager","handleLLMStart","name","_llmType","verbose","output","_generate","err","handleLLMError","handleLLMEnd","Array","isArray","Error","params","serialize","llmStringKey","entries","sort","missingPromptIndices","generations","Promise","all","prompt","index","result","lookup","push","llmOutput","length","results","i","generation","promptIndex","update","call","text","_identifyingParams","_type","_model","_modelType","deserialize","data","Cls","openai","OpenAI","LLM","_call"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/node_modules/langchain/dist/llms/base.js"],"sourcesContent":["import { InMemoryCache } from \"../cache.js\";\nimport { BaseLanguageModel, } from \"../base_language/index.js\";\n/**\n * LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n    constructor({ cache, concurrency, ...rest }) {\n        super(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\n        /**\n         * The name of the LLM class\n         */\n        Object.defineProperty(this, \"name\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"cache\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        if (typeof cache === \"object\") {\n            this.cache = cache;\n        }\n        else if (cache) {\n            this.cache = InMemoryCache.global();\n        }\n        else {\n            this.cache = undefined;\n        }\n    }\n    async generatePrompt(promptValues, stop) {\n        const prompts = promptValues.map((promptValue) => promptValue.toString());\n        return this.generate(prompts, stop);\n    }\n    /** @ignore */\n    async _generateUncached(prompts, stop) {\n        await this.callbackManager.handleLLMStart({ name: this._llmType() }, prompts, this.verbose);\n        let output;\n        try {\n            output = await this._generate(prompts, stop);\n        }\n        catch (err) {\n            await this.callbackManager.handleLLMError(err, this.verbose);\n            throw err;\n        }\n        await this.callbackManager.handleLLMEnd(output, this.verbose);\n        return output;\n    }\n    /**\n     * Run the LLM on the given propmts an input, handling caching.\n     */\n    async generate(prompts, stop) {\n        if (!Array.isArray(prompts)) {\n            throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n        }\n        if (!this.cache) {\n            return this._generateUncached(prompts, stop);\n        }\n        const { cache } = this;\n        const params = this.serialize();\n        params.stop = stop;\n        const llmStringKey = `${Object.entries(params).sort()}`;\n        const missingPromptIndices = [];\n        const generations = await Promise.all(prompts.map(async (prompt, index) => {\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (!result) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => prompts[i]), stop);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                return cache.update(prompts[promptIndex], llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n     */\n    async call(prompt, stop) {\n        const { generations } = await this.generate([prompt], stop);\n        return generations[0][0].text;\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    _modelType() {\n        return \"base_llm\";\n    }\n    /**\n     * Load an LLM from a json-like object describing it.\n     */\n    static async deserialize(data) {\n        const { _type, _model, ...rest } = data;\n        if (_model && _model !== \"base_llm\") {\n            throw new Error(`Cannot load LLM with model ${_model}`);\n        }\n        const Cls = {\n            openai: (await import(\"./openai.js\")).OpenAI,\n        }[_type];\n        if (Cls === undefined) {\n            throw new Error(`Cannot load  LLM with type ${_type}`);\n        }\n        return new Cls(rest);\n    }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n    async _generate(prompts, stop) {\n        const generations = [];\n        for (let i = 0; i < prompts.length; i += 1) {\n            const text = await this._call(prompts[i], stop);\n            generations.push([{ text }]);\n        }\n        return { generations };\n    }\n}\n"],"mappings":"AAAA,SAASA,aAAa,QAAQ,aAAa;AAC3C,SAASC,iBAAiB,QAAS,2BAA2B;AAC9D;AACA;AACA;AACA,OAAO,MAAMC,OAAO,SAASD,iBAAiB,CAAC;EAC3CE,WAAWA,CAAAC,IAAA,EAAkC;IAAA,IAAjC;MAAEC,KAAK;MAAEC,WAAW;MAAE,GAAGC;IAAK,CAAC,GAAAH,IAAA;IACvC,KAAK,CAACE,WAAW,GAAG;MAAEE,cAAc,EAAEF,WAAW;MAAE,GAAGC;IAAK,CAAC,GAAGA,IAAI,CAAC;IACpE;AACR;AACA;IACQE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,OAAO,EAAE;MACjCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,OAAOT,KAAK,KAAK,QAAQ,EAAE;MAC3B,IAAI,CAACA,KAAK,GAAGA,KAAK;IACtB,CAAC,MACI,IAAIA,KAAK,EAAE;MACZ,IAAI,CAACA,KAAK,GAAGL,aAAa,CAACe,MAAM,EAAE;IACvC,CAAC,MACI;MACD,IAAI,CAACV,KAAK,GAAGW,SAAS;IAC1B;EACJ;EACA,MAAMC,cAAcA,CAACC,YAAY,EAAEC,IAAI,EAAE;IACrC,MAAMC,OAAO,GAAGF,YAAY,CAACG,GAAG,CAAEC,WAAW,IAAKA,WAAW,CAACC,QAAQ,EAAE,CAAC;IACzE,OAAO,IAAI,CAACC,QAAQ,CAACJ,OAAO,EAAED,IAAI,CAAC;EACvC;EACA;EACA,MAAMM,iBAAiBA,CAACL,OAAO,EAAED,IAAI,EAAE;IACnC,MAAM,IAAI,CAACO,eAAe,CAACC,cAAc,CAAC;MAAEC,IAAI,EAAE,IAAI,CAACC,QAAQ;IAAG,CAAC,EAAET,OAAO,EAAE,IAAI,CAACU,OAAO,CAAC;IAC3F,IAAIC,MAAM;IACV,IAAI;MACAA,MAAM,GAAG,MAAM,IAAI,CAACC,SAAS,CAACZ,OAAO,EAAED,IAAI,CAAC;IAChD,CAAC,CACD,OAAOc,GAAG,EAAE;MACR,MAAM,IAAI,CAACP,eAAe,CAACQ,cAAc,CAACD,GAAG,EAAE,IAAI,CAACH,OAAO,CAAC;MAC5D,MAAMG,GAAG;IACb;IACA,MAAM,IAAI,CAACP,eAAe,CAACS,YAAY,CAACJ,MAAM,EAAE,IAAI,CAACD,OAAO,CAAC;IAC7D,OAAOC,MAAM;EACjB;EACA;AACJ;AACA;EACI,MAAMP,QAAQA,CAACJ,OAAO,EAAED,IAAI,EAAE;IAC1B,IAAI,CAACiB,KAAK,CAACC,OAAO,CAACjB,OAAO,CAAC,EAAE;MACzB,MAAM,IAAIkB,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,IAAI,CAAC,IAAI,CAACjC,KAAK,EAAE;MACb,OAAO,IAAI,CAACoB,iBAAiB,CAACL,OAAO,EAAED,IAAI,CAAC;IAChD;IACA,MAAM;MAAEd;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMkC,MAAM,GAAG,IAAI,CAACC,SAAS,EAAE;IAC/BD,MAAM,CAACpB,IAAI,GAAGA,IAAI;IAClB,MAAMsB,YAAY,GAAI,GAAEhC,MAAM,CAACiC,OAAO,CAACH,MAAM,CAAC,CAACI,IAAI,EAAG,EAAC;IACvD,MAAMC,oBAAoB,GAAG,EAAE;IAC/B,MAAMC,WAAW,GAAG,MAAMC,OAAO,CAACC,GAAG,CAAC3B,OAAO,CAACC,GAAG,CAAC,OAAO2B,MAAM,EAAEC,KAAK,KAAK;MACvE,MAAMC,MAAM,GAAG,MAAM7C,KAAK,CAAC8C,MAAM,CAACH,MAAM,EAAEP,YAAY,CAAC;MACvD,IAAI,CAACS,MAAM,EAAE;QACTN,oBAAoB,CAACQ,IAAI,CAACH,KAAK,CAAC;MACpC;MACA,OAAOC,MAAM;IACjB,CAAC,CAAC,CAAC;IACH,IAAIG,SAAS,GAAG,CAAC,CAAC;IAClB,IAAIT,oBAAoB,CAACU,MAAM,GAAG,CAAC,EAAE;MACjC,MAAMC,OAAO,GAAG,MAAM,IAAI,CAAC9B,iBAAiB,CAACmB,oBAAoB,CAACvB,GAAG,CAAEmC,CAAC,IAAKpC,OAAO,CAACoC,CAAC,CAAC,CAAC,EAAErC,IAAI,CAAC;MAC/F,MAAM2B,OAAO,CAACC,GAAG,CAACQ,OAAO,CAACV,WAAW,CAACxB,GAAG,CAAC,OAAOoC,UAAU,EAAER,KAAK,KAAK;QACnE,MAAMS,WAAW,GAAGd,oBAAoB,CAACK,KAAK,CAAC;QAC/CJ,WAAW,CAACa,WAAW,CAAC,GAAGD,UAAU;QACrC,OAAOpD,KAAK,CAACsD,MAAM,CAACvC,OAAO,CAACsC,WAAW,CAAC,EAAEjB,YAAY,EAAEgB,UAAU,CAAC;MACvE,CAAC,CAAC,CAAC;MACHJ,SAAS,GAAGE,OAAO,CAACF,SAAS,IAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAER,WAAW;MAAEQ;IAAU,CAAC;EACrC;EACA;AACJ;AACA;EACI,MAAMO,IAAIA,CAACZ,MAAM,EAAE7B,IAAI,EAAE;IACrB,MAAM;MAAE0B;IAAY,CAAC,GAAG,MAAM,IAAI,CAACrB,QAAQ,CAAC,CAACwB,MAAM,CAAC,EAAE7B,IAAI,CAAC;IAC3D,OAAO0B,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACgB,IAAI;EACjC;EACA;AACJ;AACA;EACI;EACAC,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;EACItB,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACsB,kBAAkB,EAAE;MAC5BC,KAAK,EAAE,IAAI,CAAClC,QAAQ,EAAE;MACtBmC,MAAM,EAAE,IAAI,CAACC,UAAU;IAC3B,CAAC;EACL;EACAA,UAAUA,CAAA,EAAG;IACT,OAAO,UAAU;EACrB;EACA;AACJ;AACA;EACI,aAAaC,WAAWA,CAACC,IAAI,EAAE;IAC3B,MAAM;MAAEJ,KAAK;MAAEC,MAAM;MAAE,GAAGzD;IAAK,CAAC,GAAG4D,IAAI;IACvC,IAAIH,MAAM,IAAIA,MAAM,KAAK,UAAU,EAAE;MACjC,MAAM,IAAI1B,KAAK,CAAE,8BAA6B0B,MAAO,EAAC,CAAC;IAC3D;IACA,MAAMI,GAAG,GAAG;MACRC,MAAM,EAAE,CAAC,MAAM,MAAM,CAAC,aAAa,CAAC,EAAEC;IAC1C,CAAC,CAACP,KAAK,CAAC;IACR,IAAIK,GAAG,KAAKpD,SAAS,EAAE;MACnB,MAAM,IAAIsB,KAAK,CAAE,8BAA6ByB,KAAM,EAAC,CAAC;IAC1D;IACA,OAAO,IAAIK,GAAG,CAAC7D,IAAI,CAAC;EACxB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMgE,GAAG,SAASrE,OAAO,CAAC;EAC7B,MAAM8B,SAASA,CAACZ,OAAO,EAAED,IAAI,EAAE;IAC3B,MAAM0B,WAAW,GAAG,EAAE;IACtB,KAAK,IAAIW,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGpC,OAAO,CAACkC,MAAM,EAAEE,CAAC,IAAI,CAAC,EAAE;MACxC,MAAMK,IAAI,GAAG,MAAM,IAAI,CAACW,KAAK,CAACpD,OAAO,CAACoC,CAAC,CAAC,EAAErC,IAAI,CAAC;MAC/C0B,WAAW,CAACO,IAAI,CAAC,CAAC;QAAES;MAAK,CAAC,CAAC,CAAC;IAChC;IACA,OAAO;MAAEhB;IAAY,CAAC;EAC1B;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
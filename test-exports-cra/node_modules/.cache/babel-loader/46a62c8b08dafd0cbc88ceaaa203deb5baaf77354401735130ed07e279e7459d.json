{"ast":null,"code":"import { Configuration, OpenAIApi } from \"openai\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { BaseChatModel } from \"./base.js\";\nimport { AIChatMessage, ChatMessage, HumanChatMessage, SystemChatMessage } from \"../schema/index.js\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nfunction messageTypeToOpenAIRole(type) {\n  switch (type) {\n    case \"system\":\n      return \"system\";\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\nfunction openAIResponseToChatMessage(role, text) {\n  switch (role) {\n    case \"user\":\n      return new HumanChatMessage(text);\n    case \"assistant\":\n      return new AIChatMessage(text);\n    case \"system\":\n      return new SystemChatMessage(text);\n    default:\n      return new ChatMessage(text, role ?? \"unknown\");\n  }\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments OpenAIInput\n */\nexport class ChatOpenAI extends BaseChatModel {\n  constructor(fields, configuration) {\n    super(fields ?? {});\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"frequencyPenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"presencePenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"n\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"logitBias\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"gpt-3.5-turbo\"\n    });\n    Object.defineProperty(this, \"modelKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stop\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"timeout\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"maxTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"clientConfig\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    const apiKey = fields?.openAIApiKey ?? (\n    // eslint-disable-next-line no-process-env\n    typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\n    if (!apiKey) {\n      throw new Error(\"OpenAI API key not found\");\n    }\n    this.modelName = fields?.modelName ?? this.modelName;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.timeout = fields?.timeout;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.maxTokens = fields?.maxTokens;\n    this.n = fields?.n ?? this.n;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stop;\n    this.streaming = fields?.streaming ?? false;\n    if (this.streaming && this.n > 1) {\n      throw new Error(\"Cannot stream results when n > 1\");\n    }\n    this.clientConfig = {\n      apiKey,\n      ...configuration\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams() {\n    return {\n      model: this.modelName,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      max_tokens: this.maxTokens,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: this.stop,\n      stream: this.streaming,\n      ...this.modelKwargs\n    };\n  }\n  _identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams(),\n      ...this.clientConfig\n    };\n  }\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n  /**\n   * Call out to OpenAI's endpoint with k unique prompts\n   *\n   * @param messages - The messages to pass into the model.\n   * @param [stop] - Optional list of stop words to use when generating.\n   *\n   * @returns The full LLM output.\n   *\n   * @example\n   * ```ts\n   * import { OpenAI } from \"langchain/llms/openai\";\n   * const openai = new OpenAI();\n   * const response = await openai.generate([\"Tell me a joke.\"]);\n   * ```\n   */\n  async _generate(messages, stop) {\n    const tokenUsage = {};\n    if (this.stop && stop) {\n      throw new Error(\"Stop found in input and default params\");\n    }\n    const params = this.invocationParams();\n    params.stop = stop ?? params.stop;\n    const messagesMapped = messages.map(message => ({\n      role: messageTypeToOpenAIRole(message._getType()),\n      content: message.text,\n      name: message.name\n    }));\n    const data = params.stream ? await new Promise((resolve, reject) => {\n      let response;\n      let rejected = false;\n      this.completionWithRetry({\n        ...params,\n        messages: messagesMapped\n      }, {\n        responseType: \"stream\",\n        onmessage: event => {\n          if (event.data?.trim?.() === \"[DONE]\") {\n            resolve(response);\n          } else {\n            const message = JSON.parse(event.data);\n            // on the first message set the response properties\n            if (!response) {\n              response = {\n                id: message.id,\n                object: message.object,\n                created: message.created,\n                model: message.model,\n                choices: []\n              };\n            }\n            // on all messages, update choice\n            const part = message.choices[0];\n            if (part != null) {\n              let choice = response.choices.find(c => c.index === part.index);\n              if (!choice) {\n                choice = {\n                  index: part.index,\n                  finish_reason: part.finish_reason ?? undefined\n                };\n                response.choices.push(choice);\n              }\n              if (!choice.message) {\n                choice.message = {\n                  role: part.delta?.role,\n                  content: part.delta?.content ?? \"\"\n                };\n              }\n              choice.message.content += part.delta?.content ?? \"\";\n              // eslint-disable-next-line no-void\n              void this.callbackManager.handleLLMNewToken(part.delta?.content ?? \"\", true);\n            }\n          }\n        }\n      }).catch(error => {\n        if (!rejected) {\n          rejected = true;\n          reject(error);\n        }\n      });\n    }) : await this.completionWithRetry({\n      ...params,\n      messages: messagesMapped\n    });\n    const {\n      completion_tokens: completionTokens,\n      prompt_tokens: promptTokens,\n      total_tokens: totalTokens\n    } = data.usage ?? {};\n    if (completionTokens) {\n      tokenUsage.completionTokens = (tokenUsage.completionTokens ?? 0) + completionTokens;\n    }\n    if (promptTokens) {\n      tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n    }\n    if (totalTokens) {\n      tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n    }\n    const generations = [];\n    for (const part of data.choices) {\n      const role = part.message?.role ?? undefined;\n      const text = part.message?.content ?? \"\";\n      generations.push({\n        text,\n        message: openAIResponseToChatMessage(role, text)\n      });\n    }\n    return {\n      generations,\n      llmOutput: {\n        tokenUsage\n      }\n    };\n  }\n  async getNumTokensFromMessages(messages) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n    const countPerMessage = await Promise.all(messages.map(async message => {\n      const textCount = await this.getNumTokens(message.text);\n      const count = textCount + tokensPerMessage + (message.name ? tokensPerName : 0);\n      totalCount += count;\n      return count;\n    }));\n    return {\n      totalCount,\n      countPerMessage\n    };\n  }\n  /** @ignore */\n  async completionWithRetry(request, options) {\n    if (!this.client) {\n      const clientConfig = new Configuration({\n        ...this.clientConfig,\n        baseOptions: {\n          timeout: this.timeout,\n          adapter: fetchAdapter,\n          ...this.clientConfig.baseOptions\n        }\n      });\n      this.client = new OpenAIApi(clientConfig);\n    }\n    return this.caller.call(this.client.createChatCompletion.bind(this.client), request, options).then(res => res.data);\n  }\n  _llmType() {\n    return \"openai\";\n  }\n  _combineLLMOutput() {\n    for (var _len = arguments.length, llmOutputs = new Array(_len), _key = 0; _key < _len; _key++) {\n      llmOutputs[_key] = arguments[_key];\n    }\n    return llmOutputs.reduce((acc, llmOutput) => {\n      if (llmOutput && llmOutput.tokenUsage) {\n        acc.tokenUsage.completionTokens += llmOutput.tokenUsage.completionTokens ?? 0;\n        acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n        acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n      }\n      return acc;\n    }, {\n      tokenUsage: {\n        completionTokens: 0,\n        promptTokens: 0,\n        totalTokens: 0\n      }\n    });\n  }\n}","map":{"version":3,"names":["Configuration","OpenAIApi","fetchAdapter","BaseChatModel","AIChatMessage","ChatMessage","HumanChatMessage","SystemChatMessage","getModelNameForTiktoken","messageTypeToOpenAIRole","type","Error","openAIResponseToChatMessage","role","text","ChatOpenAI","constructor","fields","configuration","Object","defineProperty","enumerable","configurable","writable","value","apiKey","openAIApiKey","process","env","OPENAI_API_KEY","undefined","modelName","modelKwargs","timeout","temperature","topP","frequencyPenalty","presencePenalty","maxTokens","n","logitBias","stop","streaming","clientConfig","invocationParams","model","top_p","frequency_penalty","presence_penalty","max_tokens","logit_bias","stream","_identifyingParams","model_name","identifyingParams","_generate","messages","tokenUsage","params","messagesMapped","map","message","_getType","content","name","data","Promise","resolve","reject","response","rejected","completionWithRetry","responseType","onmessage","event","trim","JSON","parse","id","object","created","choices","part","choice","find","c","index","finish_reason","push","delta","callbackManager","handleLLMNewToken","catch","error","completion_tokens","completionTokens","prompt_tokens","promptTokens","total_tokens","totalTokens","usage","generations","llmOutput","getNumTokensFromMessages","totalCount","tokensPerMessage","tokensPerName","startsWith","countPerMessage","all","textCount","getNumTokens","count","request","options","client","baseOptions","adapter","caller","call","createChatCompletion","bind","then","res","_llmType","_combineLLMOutput","_len","arguments","length","llmOutputs","Array","_key","reduce","acc"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/node_modules/langchain/dist/chat_models/openai.js"],"sourcesContent":["import { Configuration, OpenAIApi, } from \"openai\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { BaseChatModel } from \"./base.js\";\nimport { AIChatMessage, ChatMessage, HumanChatMessage, SystemChatMessage, } from \"../schema/index.js\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nfunction messageTypeToOpenAIRole(type) {\n    switch (type) {\n        case \"system\":\n            return \"system\";\n        case \"ai\":\n            return \"assistant\";\n        case \"human\":\n            return \"user\";\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\nfunction openAIResponseToChatMessage(role, text) {\n    switch (role) {\n        case \"user\":\n            return new HumanChatMessage(text);\n        case \"assistant\":\n            return new AIChatMessage(text);\n        case \"system\":\n            return new SystemChatMessage(text);\n        default:\n            return new ChatMessage(text, role ?? \"unknown\");\n    }\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments OpenAIInput\n */\nexport class ChatOpenAI extends BaseChatModel {\n    constructor(fields, configuration) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const apiKey = fields?.openAIApiKey ??\n            // eslint-disable-next-line no-process-env\n            (typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\n        if (!apiKey) {\n            throw new Error(\"OpenAI API key not found\");\n        }\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.maxTokens = fields?.maxTokens;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.streaming = fields?.streaming ?? false;\n        if (this.streaming && this.n > 1) {\n            throw new Error(\"Cannot stream results when n > 1\");\n        }\n        this.clientConfig = {\n            apiKey,\n            ...configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams() {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            max_tokens: this.maxTokens,\n            n: this.n,\n            logit_bias: this.logitBias,\n            stop: this.stop,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /**\n     * Call out to OpenAI's endpoint with k unique prompts\n     *\n     * @param messages - The messages to pass into the model.\n     * @param [stop] - Optional list of stop words to use when generating.\n     *\n     * @returns The full LLM output.\n     *\n     * @example\n     * ```ts\n     * import { OpenAI } from \"langchain/llms/openai\";\n     * const openai = new OpenAI();\n     * const response = await openai.generate([\"Tell me a joke.\"]);\n     * ```\n     */\n    async _generate(messages, stop) {\n        const tokenUsage = {};\n        if (this.stop && stop) {\n            throw new Error(\"Stop found in input and default params\");\n        }\n        const params = this.invocationParams();\n        params.stop = stop ?? params.stop;\n        const messagesMapped = messages.map((message) => ({\n            role: messageTypeToOpenAIRole(message._getType()),\n            content: message.text,\n            name: message.name,\n        }));\n        const data = params.stream\n            ? await new Promise((resolve, reject) => {\n                let response;\n                let rejected = false;\n                this.completionWithRetry({\n                    ...params,\n                    messages: messagesMapped,\n                }, {\n                    responseType: \"stream\",\n                    onmessage: (event) => {\n                        if (event.data?.trim?.() === \"[DONE]\") {\n                            resolve(response);\n                        }\n                        else {\n                            const message = JSON.parse(event.data);\n                            // on the first message set the response properties\n                            if (!response) {\n                                response = {\n                                    id: message.id,\n                                    object: message.object,\n                                    created: message.created,\n                                    model: message.model,\n                                    choices: [],\n                                };\n                            }\n                            // on all messages, update choice\n                            const part = message.choices[0];\n                            if (part != null) {\n                                let choice = response.choices.find((c) => c.index === part.index);\n                                if (!choice) {\n                                    choice = {\n                                        index: part.index,\n                                        finish_reason: part.finish_reason ?? undefined,\n                                    };\n                                    response.choices.push(choice);\n                                }\n                                if (!choice.message) {\n                                    choice.message = {\n                                        role: part.delta\n                                            ?.role,\n                                        content: part.delta?.content ?? \"\",\n                                    };\n                                }\n                                choice.message.content += part.delta?.content ?? \"\";\n                                // eslint-disable-next-line no-void\n                                void this.callbackManager.handleLLMNewToken(part.delta?.content ?? \"\", true);\n                            }\n                        }\n                    },\n                }).catch((error) => {\n                    if (!rejected) {\n                        rejected = true;\n                        reject(error);\n                    }\n                });\n            })\n            : await this.completionWithRetry({\n                ...params,\n                messages: messagesMapped,\n            });\n        const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage ?? {};\n        if (completionTokens) {\n            tokenUsage.completionTokens =\n                (tokenUsage.completionTokens ?? 0) + completionTokens;\n        }\n        if (promptTokens) {\n            tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n        }\n        if (totalTokens) {\n            tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n        }\n        const generations = [];\n        for (const part of data.choices) {\n            const role = part.message?.role ?? undefined;\n            const text = part.message?.content ?? \"\";\n            generations.push({\n                text,\n                message: openAIResponseToChatMessage(role, text),\n            });\n        }\n        return {\n            generations,\n            llmOutput: { tokenUsage },\n        };\n    }\n    async getNumTokensFromMessages(messages) {\n        let totalCount = 0;\n        let tokensPerMessage = 0;\n        let tokensPerName = 0;\n        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        if (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\n            tokensPerMessage = 4;\n            tokensPerName = -1;\n        }\n        else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\n            tokensPerMessage = 3;\n            tokensPerName = 1;\n        }\n        const countPerMessage = await Promise.all(messages.map(async (message) => {\n            const textCount = await this.getNumTokens(message.text);\n            const count = textCount + tokensPerMessage + (message.name ? tokensPerName : 0);\n            totalCount += count;\n            return count;\n        }));\n        return { totalCount, countPerMessage };\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const clientConfig = new Configuration({\n                ...this.clientConfig,\n                baseOptions: {\n                    timeout: this.timeout,\n                    adapter: fetchAdapter,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new OpenAIApi(clientConfig);\n        }\n        return this.caller\n            .call(this.client.createChatCompletion.bind(this.client), request, options)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n    _combineLLMOutput(...llmOutputs) {\n        return llmOutputs.reduce((acc, llmOutput) => {\n            if (llmOutput && llmOutput.tokenUsage) {\n                acc.tokenUsage.completionTokens +=\n                    llmOutput.tokenUsage.completionTokens ?? 0;\n                acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n                acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n            }\n            return acc;\n        }, {\n            tokenUsage: {\n                completionTokens: 0,\n                promptTokens: 0,\n                totalTokens: 0,\n            },\n        });\n    }\n}\n"],"mappings":"AAAA,SAASA,aAAa,EAAEC,SAAS,QAAS,QAAQ;AAClD,OAAOC,YAAY,MAAM,gCAAgC;AACzD,SAASC,aAAa,QAAQ,WAAW;AACzC,SAASC,aAAa,EAAEC,WAAW,EAAEC,gBAAgB,EAAEC,iBAAiB,QAAS,oBAAoB;AACrG,SAASC,uBAAuB,QAAQ,kCAAkC;AAC1E,SAASC,uBAAuBA,CAACC,IAAI,EAAE;EACnC,QAAQA,IAAI;IACR,KAAK,QAAQ;MACT,OAAO,QAAQ;IACnB,KAAK,IAAI;MACL,OAAO,WAAW;IACtB,KAAK,OAAO;MACR,OAAO,MAAM;IACjB;MACI,MAAM,IAAIC,KAAK,CAAE,yBAAwBD,IAAK,EAAC,CAAC;EAAC;AAE7D;AACA,SAASE,2BAA2BA,CAACC,IAAI,EAAEC,IAAI,EAAE;EAC7C,QAAQD,IAAI;IACR,KAAK,MAAM;MACP,OAAO,IAAIP,gBAAgB,CAACQ,IAAI,CAAC;IACrC,KAAK,WAAW;MACZ,OAAO,IAAIV,aAAa,CAACU,IAAI,CAAC;IAClC,KAAK,QAAQ;MACT,OAAO,IAAIP,iBAAiB,CAACO,IAAI,CAAC;IACtC;MACI,OAAO,IAAIT,WAAW,CAACS,IAAI,EAAED,IAAI,IAAI,SAAS,CAAC;EAAC;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,UAAU,SAASZ,aAAa,CAAC;EAC1Ca,WAAWA,CAACC,MAAM,EAAEC,aAAa,EAAE;IAC/B,KAAK,CAACD,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,GAAG,EAAE;MAC7BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,MAAMC,MAAM,GAAGR,MAAM,EAAES,YAAY;IAC/B;IACC,OAAOC,OAAO,KAAK,WAAW,GAAGA,OAAO,CAACC,GAAG,CAACC,cAAc,GAAGC,SAAS,CAAC;IAC7E,IAAI,CAACL,MAAM,EAAE;MACT,MAAM,IAAId,KAAK,CAAC,0BAA0B,CAAC;IAC/C;IACA,IAAI,CAACoB,SAAS,GAAGd,MAAM,EAAEc,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,WAAW,GAAGf,MAAM,EAAEe,WAAW,IAAI,CAAC,CAAC;IAC5C,IAAI,CAACC,OAAO,GAAGhB,MAAM,EAAEgB,OAAO;IAC9B,IAAI,CAACC,WAAW,GAAGjB,MAAM,EAAEiB,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,IAAI,GAAGlB,MAAM,EAAEkB,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,gBAAgB,GAAGnB,MAAM,EAAEmB,gBAAgB,IAAI,IAAI,CAACA,gBAAgB;IACzE,IAAI,CAACC,eAAe,GAAGpB,MAAM,EAAEoB,eAAe,IAAI,IAAI,CAACA,eAAe;IACtE,IAAI,CAACC,SAAS,GAAGrB,MAAM,EAAEqB,SAAS;IAClC,IAAI,CAACC,CAAC,GAAGtB,MAAM,EAAEsB,CAAC,IAAI,IAAI,CAACA,CAAC;IAC5B,IAAI,CAACC,SAAS,GAAGvB,MAAM,EAAEuB,SAAS;IAClC,IAAI,CAACC,IAAI,GAAGxB,MAAM,EAAEwB,IAAI;IACxB,IAAI,CAACC,SAAS,GAAGzB,MAAM,EAAEyB,SAAS,IAAI,KAAK;IAC3C,IAAI,IAAI,CAACA,SAAS,IAAI,IAAI,CAACH,CAAC,GAAG,CAAC,EAAE;MAC9B,MAAM,IAAI5B,KAAK,CAAC,kCAAkC,CAAC;IACvD;IACA,IAAI,CAACgC,YAAY,GAAG;MAChBlB,MAAM;MACN,GAAGP;IACP,CAAC;EACL;EACA;AACJ;AACA;EACI0B,gBAAgBA,CAAA,EAAG;IACf,OAAO;MACHC,KAAK,EAAE,IAAI,CAACd,SAAS;MACrBG,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BY,KAAK,EAAE,IAAI,CAACX,IAAI;MAChBY,iBAAiB,EAAE,IAAI,CAACX,gBAAgB;MACxCY,gBAAgB,EAAE,IAAI,CAACX,eAAe;MACtCY,UAAU,EAAE,IAAI,CAACX,SAAS;MAC1BC,CAAC,EAAE,IAAI,CAACA,CAAC;MACTW,UAAU,EAAE,IAAI,CAACV,SAAS;MAC1BC,IAAI,EAAE,IAAI,CAACA,IAAI;MACfU,MAAM,EAAE,IAAI,CAACT,SAAS;MACtB,GAAG,IAAI,CAACV;IACZ,CAAC;EACL;EACAoB,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHC,UAAU,EAAE,IAAI,CAACtB,SAAS;MAC1B,GAAG,IAAI,CAACa,gBAAgB,EAAE;MAC1B,GAAG,IAAI,CAACD;IACZ,CAAC;EACL;EACA;AACJ;AACA;EACIW,iBAAiBA,CAAA,EAAG;IAChB,OAAO,IAAI,CAACF,kBAAkB,EAAE;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMG,SAASA,CAACC,QAAQ,EAAEf,IAAI,EAAE;IAC5B,MAAMgB,UAAU,GAAG,CAAC,CAAC;IACrB,IAAI,IAAI,CAAChB,IAAI,IAAIA,IAAI,EAAE;MACnB,MAAM,IAAI9B,KAAK,CAAC,wCAAwC,CAAC;IAC7D;IACA,MAAM+C,MAAM,GAAG,IAAI,CAACd,gBAAgB,EAAE;IACtCc,MAAM,CAACjB,IAAI,GAAGA,IAAI,IAAIiB,MAAM,CAACjB,IAAI;IACjC,MAAMkB,cAAc,GAAGH,QAAQ,CAACI,GAAG,CAAEC,OAAO,KAAM;MAC9ChD,IAAI,EAAEJ,uBAAuB,CAACoD,OAAO,CAACC,QAAQ,EAAE,CAAC;MACjDC,OAAO,EAAEF,OAAO,CAAC/C,IAAI;MACrBkD,IAAI,EAAEH,OAAO,CAACG;IAClB,CAAC,CAAC,CAAC;IACH,MAAMC,IAAI,GAAGP,MAAM,CAACP,MAAM,GACpB,MAAM,IAAIe,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;MACrC,IAAIC,QAAQ;MACZ,IAAIC,QAAQ,GAAG,KAAK;MACpB,IAAI,CAACC,mBAAmB,CAAC;QACrB,GAAGb,MAAM;QACTF,QAAQ,EAAEG;MACd,CAAC,EAAE;QACCa,YAAY,EAAE,QAAQ;QACtBC,SAAS,EAAGC,KAAK,IAAK;UAClB,IAAIA,KAAK,CAACT,IAAI,EAAEU,IAAI,IAAI,KAAK,QAAQ,EAAE;YACnCR,OAAO,CAACE,QAAQ,CAAC;UACrB,CAAC,MACI;YACD,MAAMR,OAAO,GAAGe,IAAI,CAACC,KAAK,CAACH,KAAK,CAACT,IAAI,CAAC;YACtC;YACA,IAAI,CAACI,QAAQ,EAAE;cACXA,QAAQ,GAAG;gBACPS,EAAE,EAAEjB,OAAO,CAACiB,EAAE;gBACdC,MAAM,EAAElB,OAAO,CAACkB,MAAM;gBACtBC,OAAO,EAAEnB,OAAO,CAACmB,OAAO;gBACxBnC,KAAK,EAAEgB,OAAO,CAAChB,KAAK;gBACpBoC,OAAO,EAAE;cACb,CAAC;YACL;YACA;YACA,MAAMC,IAAI,GAAGrB,OAAO,CAACoB,OAAO,CAAC,CAAC,CAAC;YAC/B,IAAIC,IAAI,IAAI,IAAI,EAAE;cACd,IAAIC,MAAM,GAAGd,QAAQ,CAACY,OAAO,CAACG,IAAI,CAAEC,CAAC,IAAKA,CAAC,CAACC,KAAK,KAAKJ,IAAI,CAACI,KAAK,CAAC;cACjE,IAAI,CAACH,MAAM,EAAE;gBACTA,MAAM,GAAG;kBACLG,KAAK,EAAEJ,IAAI,CAACI,KAAK;kBACjBC,aAAa,EAAEL,IAAI,CAACK,aAAa,IAAIzD;gBACzC,CAAC;gBACDuC,QAAQ,CAACY,OAAO,CAACO,IAAI,CAACL,MAAM,CAAC;cACjC;cACA,IAAI,CAACA,MAAM,CAACtB,OAAO,EAAE;gBACjBsB,MAAM,CAACtB,OAAO,GAAG;kBACbhD,IAAI,EAAEqE,IAAI,CAACO,KAAK,EACV5E,IAAI;kBACVkD,OAAO,EAAEmB,IAAI,CAACO,KAAK,EAAE1B,OAAO,IAAI;gBACpC,CAAC;cACL;cACAoB,MAAM,CAACtB,OAAO,CAACE,OAAO,IAAImB,IAAI,CAACO,KAAK,EAAE1B,OAAO,IAAI,EAAE;cACnD;cACA,KAAK,IAAI,CAAC2B,eAAe,CAACC,iBAAiB,CAACT,IAAI,CAACO,KAAK,EAAE1B,OAAO,IAAI,EAAE,EAAE,IAAI,CAAC;YAChF;UACJ;QACJ;MACJ,CAAC,CAAC,CAAC6B,KAAK,CAAEC,KAAK,IAAK;QAChB,IAAI,CAACvB,QAAQ,EAAE;UACXA,QAAQ,GAAG,IAAI;UACfF,MAAM,CAACyB,KAAK,CAAC;QACjB;MACJ,CAAC,CAAC;IACN,CAAC,CAAC,GACA,MAAM,IAAI,CAACtB,mBAAmB,CAAC;MAC7B,GAAGb,MAAM;MACTF,QAAQ,EAAEG;IACd,CAAC,CAAC;IACN,MAAM;MAAEmC,iBAAiB,EAAEC,gBAAgB;MAAEC,aAAa,EAAEC,YAAY;MAAEC,YAAY,EAAEC;IAAa,CAAC,GAAGlC,IAAI,CAACmC,KAAK,IAAI,CAAC,CAAC;IACzH,IAAIL,gBAAgB,EAAE;MAClBtC,UAAU,CAACsC,gBAAgB,GACvB,CAACtC,UAAU,CAACsC,gBAAgB,IAAI,CAAC,IAAIA,gBAAgB;IAC7D;IACA,IAAIE,YAAY,EAAE;MACdxC,UAAU,CAACwC,YAAY,GAAG,CAACxC,UAAU,CAACwC,YAAY,IAAI,CAAC,IAAIA,YAAY;IAC3E;IACA,IAAIE,WAAW,EAAE;MACb1C,UAAU,CAAC0C,WAAW,GAAG,CAAC1C,UAAU,CAAC0C,WAAW,IAAI,CAAC,IAAIA,WAAW;IACxE;IACA,MAAME,WAAW,GAAG,EAAE;IACtB,KAAK,MAAMnB,IAAI,IAAIjB,IAAI,CAACgB,OAAO,EAAE;MAC7B,MAAMpE,IAAI,GAAGqE,IAAI,CAACrB,OAAO,EAAEhD,IAAI,IAAIiB,SAAS;MAC5C,MAAMhB,IAAI,GAAGoE,IAAI,CAACrB,OAAO,EAAEE,OAAO,IAAI,EAAE;MACxCsC,WAAW,CAACb,IAAI,CAAC;QACb1E,IAAI;QACJ+C,OAAO,EAAEjD,2BAA2B,CAACC,IAAI,EAAEC,IAAI;MACnD,CAAC,CAAC;IACN;IACA,OAAO;MACHuF,WAAW;MACXC,SAAS,EAAE;QAAE7C;MAAW;IAC5B,CAAC;EACL;EACA,MAAM8C,wBAAwBA,CAAC/C,QAAQ,EAAE;IACrC,IAAIgD,UAAU,GAAG,CAAC;IAClB,IAAIC,gBAAgB,GAAG,CAAC;IACxB,IAAIC,aAAa,GAAG,CAAC;IACrB;IACA,IAAIlG,uBAAuB,CAAC,IAAI,CAACuB,SAAS,CAAC,KAAK,eAAe,EAAE;MAC7D0E,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC,CAAC;IACtB,CAAC,MACI,IAAIlG,uBAAuB,CAAC,IAAI,CAACuB,SAAS,CAAC,CAAC4E,UAAU,CAAC,OAAO,CAAC,EAAE;MAClEF,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC;IACrB;IACA,MAAME,eAAe,GAAG,MAAM1C,OAAO,CAAC2C,GAAG,CAACrD,QAAQ,CAACI,GAAG,CAAC,MAAOC,OAAO,IAAK;MACtE,MAAMiD,SAAS,GAAG,MAAM,IAAI,CAACC,YAAY,CAAClD,OAAO,CAAC/C,IAAI,CAAC;MACvD,MAAMkG,KAAK,GAAGF,SAAS,GAAGL,gBAAgB,IAAI5C,OAAO,CAACG,IAAI,GAAG0C,aAAa,GAAG,CAAC,CAAC;MAC/EF,UAAU,IAAIQ,KAAK;MACnB,OAAOA,KAAK;IAChB,CAAC,CAAC,CAAC;IACH,OAAO;MAAER,UAAU;MAAEI;IAAgB,CAAC;EAC1C;EACA;EACA,MAAMrC,mBAAmBA,CAAC0C,OAAO,EAAEC,OAAO,EAAE;IACxC,IAAI,CAAC,IAAI,CAACC,MAAM,EAAE;MACd,MAAMxE,YAAY,GAAG,IAAI3C,aAAa,CAAC;QACnC,GAAG,IAAI,CAAC2C,YAAY;QACpByE,WAAW,EAAE;UACTnF,OAAO,EAAE,IAAI,CAACA,OAAO;UACrBoF,OAAO,EAAEnH,YAAY;UACrB,GAAG,IAAI,CAACyC,YAAY,CAACyE;QACzB;MACJ,CAAC,CAAC;MACF,IAAI,CAACD,MAAM,GAAG,IAAIlH,SAAS,CAAC0C,YAAY,CAAC;IAC7C;IACA,OAAO,IAAI,CAAC2E,MAAM,CACbC,IAAI,CAAC,IAAI,CAACJ,MAAM,CAACK,oBAAoB,CAACC,IAAI,CAAC,IAAI,CAACN,MAAM,CAAC,EAAEF,OAAO,EAAEC,OAAO,CAAC,CAC1EQ,IAAI,CAAEC,GAAG,IAAKA,GAAG,CAAC1D,IAAI,CAAC;EAChC;EACA2D,QAAQA,CAAA,EAAG;IACP,OAAO,QAAQ;EACnB;EACAC,iBAAiBA,CAAA,EAAgB;IAAA,SAAAC,IAAA,GAAAC,SAAA,CAAAC,MAAA,EAAZC,UAAU,OAAAC,KAAA,CAAAJ,IAAA,GAAAK,IAAA,MAAAA,IAAA,GAAAL,IAAA,EAAAK,IAAA;MAAVF,UAAU,CAAAE,IAAA,IAAAJ,SAAA,CAAAI,IAAA;IAAA;IAC3B,OAAOF,UAAU,CAACG,MAAM,CAAC,CAACC,GAAG,EAAE/B,SAAS,KAAK;MACzC,IAAIA,SAAS,IAAIA,SAAS,CAAC7C,UAAU,EAAE;QACnC4E,GAAG,CAAC5E,UAAU,CAACsC,gBAAgB,IAC3BO,SAAS,CAAC7C,UAAU,CAACsC,gBAAgB,IAAI,CAAC;QAC9CsC,GAAG,CAAC5E,UAAU,CAACwC,YAAY,IAAIK,SAAS,CAAC7C,UAAU,CAACwC,YAAY,IAAI,CAAC;QACrEoC,GAAG,CAAC5E,UAAU,CAAC0C,WAAW,IAAIG,SAAS,CAAC7C,UAAU,CAAC0C,WAAW,IAAI,CAAC;MACvE;MACA,OAAOkC,GAAG;IACd,CAAC,EAAE;MACC5E,UAAU,EAAE;QACRsC,gBAAgB,EAAE,CAAC;QACnBE,YAAY,EAAE,CAAC;QACfE,WAAW,EAAE;MACjB;IACJ,CAAC,CAAC;EACN;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
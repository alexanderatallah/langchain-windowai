{"ast":null,"code":"import { AI_PROMPT, HUMAN_PROMPT, Client as AnthropicApi } from \"@anthropic-ai/sdk\";\nimport { BaseChatModel } from \"./base.js\";\nimport { AIChatMessage } from \"../schema/index.js\";\nfunction getAnthropicPromptFromMessage(type) {\n  switch (type) {\n    case \"ai\":\n      return AI_PROMPT;\n    case \"human\":\n      return HUMAN_PROMPT;\n    case \"system\":\n      return \"\";\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\nconst DEFAULT_STOP_SEQUENCES = [HUMAN_PROMPT];\n/**\n * Wrapper around Anthropic large language models.\n *\n * To use you should have the `@anthropic-ai/sdk` package installed, with the\n * `ANTHROPIC_API_KEY` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://console.anthropic.com/docs/api/reference |\n * `anthropic.complete`} can be passed through {@link invocationKwargs},\n * even if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments AnthropicInput\n */\nexport class ChatAnthropic extends BaseChatModel {\n  constructor(fields) {\n    super(fields ?? {});\n    Object.defineProperty(this, \"apiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"topK\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: -1\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: -1\n    });\n    Object.defineProperty(this, \"maxTokensToSample\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 2048\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"claude-v1\"\n    });\n    Object.defineProperty(this, \"invocationKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stopSequences\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    // Used for non-streaming requests\n    Object.defineProperty(this, \"batchClient\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    // Used for streaming requests\n    Object.defineProperty(this, \"streamingClient\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.apiKey = fields?.anthropicApiKey ?? (typeof process !== \"undefined\" ?\n    // eslint-disable-next-line no-process-env\n    process.env.ANTHROPIC_API_KEY : undefined);\n    if (!this.apiKey) {\n      throw new Error(\"Anthropic API key not found\");\n    }\n    this.modelName = fields?.modelName ?? this.modelName;\n    this.invocationKwargs = fields?.invocationKwargs ?? {};\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topK = fields?.topK ?? this.topK;\n    this.topP = fields?.topP ?? this.topP;\n    this.maxTokensToSample = fields?.maxTokensToSample ?? this.maxTokensToSample;\n    this.stopSequences = fields?.stopSequences ?? this.stopSequences;\n    this.streaming = fields?.streaming ?? false;\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams() {\n    return {\n      model: this.modelName,\n      temperature: this.temperature,\n      top_k: this.topK,\n      top_p: this.topP,\n      stop_sequences: this.stopSequences ?? DEFAULT_STOP_SEQUENCES,\n      max_tokens_to_sample: this.maxTokensToSample,\n      stream: this.streaming,\n      ...this.invocationKwargs\n    };\n  }\n  _identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams()\n    };\n  }\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams()\n    };\n  }\n  formatMessagesAsPrompt(messages) {\n    return messages.map(message => {\n      const messagePrompt = getAnthropicPromptFromMessage(message._getType());\n      return `${messagePrompt} ${message.text}`;\n    }).join(\"\") + AI_PROMPT;\n  }\n  /**\n   * Call out to Anthropic's endpoint with k unique prompts\n   *\n   * @param messages - The messages to pass into the model.\n   * @param [stopSequences] - Optional list of stop sequences to use when generating.\n   *\n   * @returns The full LLM output.\n   *\n   * @example\n   * ```ts\n   * import { ChatAnthropic } from \"langchain/chat_models/openai\";\n   * const anthropic = new ChatAnthropic();\n   * const response = await anthropic.generate(new HumanChatMessage([\"Tell me a joke.\"]));\n   * ```\n   */\n  async _generate(messages, stopSequences) {\n    if (this.stopSequences && stopSequences) {\n      throw new Error(`\"stopSequence\" parameter found in input and default params`);\n    }\n    const params = this.invocationParams();\n    params.stop_sequences = stopSequences ? stopSequences.concat(DEFAULT_STOP_SEQUENCES) : params.stop_sequences;\n    const response = await this.completionWithRetry({\n      ...params,\n      prompt: this.formatMessagesAsPrompt(messages)\n    });\n    const generations = response.completion.split(AI_PROMPT).map(message => ({\n      text: message,\n      message: new AIChatMessage(message)\n    }));\n    return {\n      generations\n    };\n  }\n  /** @ignore */\n  async completionWithRetry(request) {\n    if (!this.apiKey) {\n      throw new Error(\"Missing Anthropic API key.\");\n    }\n    let makeCompletionRequest;\n    if (request.stream) {\n      if (!this.streamingClient) {\n        this.streamingClient = new AnthropicApi(this.apiKey);\n      }\n      makeCompletionRequest = async () => {\n        let currentCompletion = \"\";\n        return this.streamingClient.completeStream(request, {\n          onUpdate: data => {\n            if (data.stop_reason) {\n              return;\n            }\n            const part = data.completion;\n            if (part) {\n              const delta = part.slice(currentCompletion.length);\n              currentCompletion += delta ?? \"\";\n              // eslint-disable-next-line no-void\n              void this.callbackManager.handleLLMNewToken(delta ?? \"\", true);\n            }\n          }\n        });\n      };\n    } else {\n      if (!this.batchClient) {\n        this.batchClient = new AnthropicApi(this.apiKey);\n      }\n      makeCompletionRequest = async () => this.batchClient.complete(request);\n    }\n    return this.caller.call(makeCompletionRequest);\n  }\n  _llmType() {\n    return \"anthropic\";\n  }\n  _combineLLMOutput() {\n    return [];\n  }\n}","map":{"version":3,"names":["AI_PROMPT","HUMAN_PROMPT","Client","AnthropicApi","BaseChatModel","AIChatMessage","getAnthropicPromptFromMessage","type","Error","DEFAULT_STOP_SEQUENCES","ChatAnthropic","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","apiKey","anthropicApiKey","process","env","ANTHROPIC_API_KEY","undefined","modelName","invocationKwargs","temperature","topK","topP","maxTokensToSample","stopSequences","streaming","invocationParams","model","top_k","top_p","stop_sequences","max_tokens_to_sample","stream","_identifyingParams","model_name","identifyingParams","formatMessagesAsPrompt","messages","map","message","messagePrompt","_getType","text","join","_generate","params","concat","response","completionWithRetry","prompt","generations","completion","split","request","makeCompletionRequest","streamingClient","currentCompletion","completeStream","onUpdate","data","stop_reason","part","delta","slice","length","callbackManager","handleLLMNewToken","batchClient","complete","caller","call","_llmType","_combineLLMOutput"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/node_modules/langchain/dist/chat_models/anthropic.js"],"sourcesContent":["import { AI_PROMPT, HUMAN_PROMPT, Client as AnthropicApi, } from \"@anthropic-ai/sdk\";\nimport { BaseChatModel } from \"./base.js\";\nimport { AIChatMessage, } from \"../schema/index.js\";\nfunction getAnthropicPromptFromMessage(type) {\n    switch (type) {\n        case \"ai\":\n            return AI_PROMPT;\n        case \"human\":\n            return HUMAN_PROMPT;\n        case \"system\":\n            return \"\";\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\nconst DEFAULT_STOP_SEQUENCES = [HUMAN_PROMPT];\n/**\n * Wrapper around Anthropic large language models.\n *\n * To use you should have the `@anthropic-ai/sdk` package installed, with the\n * `ANTHROPIC_API_KEY` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://console.anthropic.com/docs/api/reference |\n * `anthropic.complete`} can be passed through {@link invocationKwargs},\n * even if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments AnthropicInput\n */\nexport class ChatAnthropic extends BaseChatModel {\n    constructor(fields) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"apiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topK\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: -1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: -1\n        });\n        Object.defineProperty(this, \"maxTokensToSample\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 2048\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"claude-v1\"\n        });\n        Object.defineProperty(this, \"invocationKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stopSequences\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        // Used for non-streaming requests\n        Object.defineProperty(this, \"batchClient\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        // Used for streaming requests\n        Object.defineProperty(this, \"streamingClient\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.apiKey =\n            fields?.anthropicApiKey ??\n                (typeof process !== \"undefined\"\n                    ? // eslint-disable-next-line no-process-env\n                        process.env.ANTHROPIC_API_KEY\n                    : undefined);\n        if (!this.apiKey) {\n            throw new Error(\"Anthropic API key not found\");\n        }\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.invocationKwargs = fields?.invocationKwargs ?? {};\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topK = fields?.topK ?? this.topK;\n        this.topP = fields?.topP ?? this.topP;\n        this.maxTokensToSample =\n            fields?.maxTokensToSample ?? this.maxTokensToSample;\n        this.stopSequences = fields?.stopSequences ?? this.stopSequences;\n        this.streaming = fields?.streaming ?? false;\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams() {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_k: this.topK,\n            top_p: this.topP,\n            stop_sequences: this.stopSequences ?? DEFAULT_STOP_SEQUENCES,\n            max_tokens_to_sample: this.maxTokensToSample,\n            stream: this.streaming,\n            ...this.invocationKwargs,\n        };\n    }\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n        };\n    }\n    formatMessagesAsPrompt(messages) {\n        return (messages\n            .map((message) => {\n            const messagePrompt = getAnthropicPromptFromMessage(message._getType());\n            return `${messagePrompt} ${message.text}`;\n        })\n            .join(\"\") + AI_PROMPT);\n    }\n    /**\n     * Call out to Anthropic's endpoint with k unique prompts\n     *\n     * @param messages - The messages to pass into the model.\n     * @param [stopSequences] - Optional list of stop sequences to use when generating.\n     *\n     * @returns The full LLM output.\n     *\n     * @example\n     * ```ts\n     * import { ChatAnthropic } from \"langchain/chat_models/openai\";\n     * const anthropic = new ChatAnthropic();\n     * const response = await anthropic.generate(new HumanChatMessage([\"Tell me a joke.\"]));\n     * ```\n     */\n    async _generate(messages, stopSequences) {\n        if (this.stopSequences && stopSequences) {\n            throw new Error(`\"stopSequence\" parameter found in input and default params`);\n        }\n        const params = this.invocationParams();\n        params.stop_sequences = stopSequences\n            ? stopSequences.concat(DEFAULT_STOP_SEQUENCES)\n            : params.stop_sequences;\n        const response = await this.completionWithRetry({\n            ...params,\n            prompt: this.formatMessagesAsPrompt(messages),\n        });\n        const generations = response.completion\n            .split(AI_PROMPT)\n            .map((message) => ({\n            text: message,\n            message: new AIChatMessage(message),\n        }));\n        return {\n            generations,\n        };\n    }\n    /** @ignore */\n    async completionWithRetry(request) {\n        if (!this.apiKey) {\n            throw new Error(\"Missing Anthropic API key.\");\n        }\n        let makeCompletionRequest;\n        if (request.stream) {\n            if (!this.streamingClient) {\n                this.streamingClient = new AnthropicApi(this.apiKey);\n            }\n            makeCompletionRequest = async () => {\n                let currentCompletion = \"\";\n                return this.streamingClient.completeStream(request, {\n                    onUpdate: (data) => {\n                        if (data.stop_reason) {\n                            return;\n                        }\n                        const part = data.completion;\n                        if (part) {\n                            const delta = part.slice(currentCompletion.length);\n                            currentCompletion += delta ?? \"\";\n                            // eslint-disable-next-line no-void\n                            void this.callbackManager.handleLLMNewToken(delta ?? \"\", true);\n                        }\n                    },\n                });\n            };\n        }\n        else {\n            if (!this.batchClient) {\n                this.batchClient = new AnthropicApi(this.apiKey);\n            }\n            makeCompletionRequest = async () => this.batchClient.complete(request);\n        }\n        return this.caller.call(makeCompletionRequest);\n    }\n    _llmType() {\n        return \"anthropic\";\n    }\n    _combineLLMOutput() {\n        return [];\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,YAAY,EAAEC,MAAM,IAAIC,YAAY,QAAS,mBAAmB;AACpF,SAASC,aAAa,QAAQ,WAAW;AACzC,SAASC,aAAa,QAAS,oBAAoB;AACnD,SAASC,6BAA6BA,CAACC,IAAI,EAAE;EACzC,QAAQA,IAAI;IACR,KAAK,IAAI;MACL,OAAOP,SAAS;IACpB,KAAK,OAAO;MACR,OAAOC,YAAY;IACvB,KAAK,QAAQ;MACT,OAAO,EAAE;IACb;MACI,MAAM,IAAIO,KAAK,CAAE,yBAAwBD,IAAK,EAAC,CAAC;EAAC;AAE7D;AACA,MAAME,sBAAsB,GAAG,CAACR,YAAY,CAAC;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMS,aAAa,SAASN,aAAa,CAAC;EAC7CO,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC;IACZ,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC;IACZ,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,eAAe,EAAE;MACzCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACF;IACAL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF;IACAL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACC,MAAM,GACPP,MAAM,EAAEQ,eAAe,KAClB,OAAOC,OAAO,KAAK,WAAW;IACzB;IACEA,OAAO,CAACC,GAAG,CAACC,iBAAiB,GAC/BC,SAAS,CAAC;IACxB,IAAI,CAAC,IAAI,CAACL,MAAM,EAAE;MACd,MAAM,IAAIX,KAAK,CAAC,6BAA6B,CAAC;IAClD;IACA,IAAI,CAACiB,SAAS,GAAGb,MAAM,EAAEa,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,gBAAgB,GAAGd,MAAM,EAAEc,gBAAgB,IAAI,CAAC,CAAC;IACtD,IAAI,CAACC,WAAW,GAAGf,MAAM,EAAEe,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,IAAI,GAAGhB,MAAM,EAAEgB,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,IAAI,GAAGjB,MAAM,EAAEiB,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,iBAAiB,GAClBlB,MAAM,EAAEkB,iBAAiB,IAAI,IAAI,CAACA,iBAAiB;IACvD,IAAI,CAACC,aAAa,GAAGnB,MAAM,EAAEmB,aAAa,IAAI,IAAI,CAACA,aAAa;IAChE,IAAI,CAACC,SAAS,GAAGpB,MAAM,EAAEoB,SAAS,IAAI,KAAK;EAC/C;EACA;AACJ;AACA;EACIC,gBAAgBA,CAAA,EAAG;IACf,OAAO;MACHC,KAAK,EAAE,IAAI,CAACT,SAAS;MACrBE,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BQ,KAAK,EAAE,IAAI,CAACP,IAAI;MAChBQ,KAAK,EAAE,IAAI,CAACP,IAAI;MAChBQ,cAAc,EAAE,IAAI,CAACN,aAAa,IAAItB,sBAAsB;MAC5D6B,oBAAoB,EAAE,IAAI,CAACR,iBAAiB;MAC5CS,MAAM,EAAE,IAAI,CAACP,SAAS;MACtB,GAAG,IAAI,CAACN;IACZ,CAAC;EACL;EACAc,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHC,UAAU,EAAE,IAAI,CAAChB,SAAS;MAC1B,GAAG,IAAI,CAACQ,gBAAgB;IAC5B,CAAC;EACL;EACA;AACJ;AACA;EACIS,iBAAiBA,CAAA,EAAG;IAChB,OAAO;MACHD,UAAU,EAAE,IAAI,CAAChB,SAAS;MAC1B,GAAG,IAAI,CAACQ,gBAAgB;IAC5B,CAAC;EACL;EACAU,sBAAsBA,CAACC,QAAQ,EAAE;IAC7B,OAAQA,QAAQ,CACXC,GAAG,CAAEC,OAAO,IAAK;MAClB,MAAMC,aAAa,GAAGzC,6BAA6B,CAACwC,OAAO,CAACE,QAAQ,EAAE,CAAC;MACvE,OAAQ,GAAED,aAAc,IAAGD,OAAO,CAACG,IAAK,EAAC;IAC7C,CAAC,CAAC,CACGC,IAAI,CAAC,EAAE,CAAC,GAAGlD,SAAS;EAC7B;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMmD,SAASA,CAACP,QAAQ,EAAEb,aAAa,EAAE;IACrC,IAAI,IAAI,CAACA,aAAa,IAAIA,aAAa,EAAE;MACrC,MAAM,IAAIvB,KAAK,CAAE,4DAA2D,CAAC;IACjF;IACA,MAAM4C,MAAM,GAAG,IAAI,CAACnB,gBAAgB,EAAE;IACtCmB,MAAM,CAACf,cAAc,GAAGN,aAAa,GAC/BA,aAAa,CAACsB,MAAM,CAAC5C,sBAAsB,CAAC,GAC5C2C,MAAM,CAACf,cAAc;IAC3B,MAAMiB,QAAQ,GAAG,MAAM,IAAI,CAACC,mBAAmB,CAAC;MAC5C,GAAGH,MAAM;MACTI,MAAM,EAAE,IAAI,CAACb,sBAAsB,CAACC,QAAQ;IAChD,CAAC,CAAC;IACF,MAAMa,WAAW,GAAGH,QAAQ,CAACI,UAAU,CAClCC,KAAK,CAAC3D,SAAS,CAAC,CAChB6C,GAAG,CAAEC,OAAO,KAAM;MACnBG,IAAI,EAAEH,OAAO;MACbA,OAAO,EAAE,IAAIzC,aAAa,CAACyC,OAAO;IACtC,CAAC,CAAC,CAAC;IACH,OAAO;MACHW;IACJ,CAAC;EACL;EACA;EACA,MAAMF,mBAAmBA,CAACK,OAAO,EAAE;IAC/B,IAAI,CAAC,IAAI,CAACzC,MAAM,EAAE;MACd,MAAM,IAAIX,KAAK,CAAC,4BAA4B,CAAC;IACjD;IACA,IAAIqD,qBAAqB;IACzB,IAAID,OAAO,CAACrB,MAAM,EAAE;MAChB,IAAI,CAAC,IAAI,CAACuB,eAAe,EAAE;QACvB,IAAI,CAACA,eAAe,GAAG,IAAI3D,YAAY,CAAC,IAAI,CAACgB,MAAM,CAAC;MACxD;MACA0C,qBAAqB,GAAG,MAAAA,CAAA,KAAY;QAChC,IAAIE,iBAAiB,GAAG,EAAE;QAC1B,OAAO,IAAI,CAACD,eAAe,CAACE,cAAc,CAACJ,OAAO,EAAE;UAChDK,QAAQ,EAAGC,IAAI,IAAK;YAChB,IAAIA,IAAI,CAACC,WAAW,EAAE;cAClB;YACJ;YACA,MAAMC,IAAI,GAAGF,IAAI,CAACR,UAAU;YAC5B,IAAIU,IAAI,EAAE;cACN,MAAMC,KAAK,GAAGD,IAAI,CAACE,KAAK,CAACP,iBAAiB,CAACQ,MAAM,CAAC;cAClDR,iBAAiB,IAAIM,KAAK,IAAI,EAAE;cAChC;cACA,KAAK,IAAI,CAACG,eAAe,CAACC,iBAAiB,CAACJ,KAAK,IAAI,EAAE,EAAE,IAAI,CAAC;YAClE;UACJ;QACJ,CAAC,CAAC;MACN,CAAC;IACL,CAAC,MACI;MACD,IAAI,CAAC,IAAI,CAACK,WAAW,EAAE;QACnB,IAAI,CAACA,WAAW,GAAG,IAAIvE,YAAY,CAAC,IAAI,CAACgB,MAAM,CAAC;MACpD;MACA0C,qBAAqB,GAAG,MAAAA,CAAA,KAAY,IAAI,CAACa,WAAW,CAACC,QAAQ,CAACf,OAAO,CAAC;IAC1E;IACA,OAAO,IAAI,CAACgB,MAAM,CAACC,IAAI,CAAChB,qBAAqB,CAAC;EAClD;EACAiB,QAAQA,CAAA,EAAG;IACP,OAAO,WAAW;EACtB;EACAC,iBAAiBA,CAAA,EAAG;IAChB,OAAO,EAAE;EACb;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
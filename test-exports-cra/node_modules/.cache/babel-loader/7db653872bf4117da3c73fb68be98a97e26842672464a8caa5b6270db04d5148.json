{"ast":null,"code":"import { LLM } from \"langchain/dist/llms/base\";\nexport class WindowAi extends LLM {\n  constructor(fields) {\n    var _fields$temperature, _fields$maxTokens, _fields$model, _fields$completionOpt;\n    super(fields !== null && fields !== void 0 ? fields : {});\n    this.temperature = 0;\n    this.maxTokens = 250;\n    this.model = void 0;\n    this.completionOptions = void 0;\n    this.globalContext = void 0;\n    this.temperature = (_fields$temperature = fields === null || fields === void 0 ? void 0 : fields.temperature) !== null && _fields$temperature !== void 0 ? _fields$temperature : this.temperature;\n    this.maxTokens = (_fields$maxTokens = fields === null || fields === void 0 ? void 0 : fields.maxTokens) !== null && _fields$maxTokens !== void 0 ? _fields$maxTokens : this.maxTokens;\n    this.model = (_fields$model = fields === null || fields === void 0 ? void 0 : fields.model) !== null && _fields$model !== void 0 ? _fields$model : this.model;\n    this.completionOptions = (_fields$completionOpt = fields === null || fields === void 0 ? void 0 : fields.completionOptions) !== null && _fields$completionOpt !== void 0 ? _fields$completionOpt : {};\n    this.globalContext = typeof window !== \"undefined\" ? window : globalThis;\n    this._ensureAiAvailable();\n  }\n  _llmType() {\n    return \"windowai\";\n  }\n  async _call(prompt, stopSequences) {\n    const input = typeof prompt === \"string\" ? {\n      prompt\n    } : {\n      messages: prompt\n    };\n    const options = {\n      ...this.completionOptions,\n      temperature: this.temperature,\n      maxTokens: this.maxTokens,\n      model: this.model,\n      stopSequences\n    };\n    try {\n      const output = await this.globalContext.ai.getCompletion(input, options);\n      return output.text;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not generate response from WindowAi.\");\n    }\n  }\n  async getCurrentModel() {\n    try {\n      const modelID = await this.globalContext.ai.getCurrentModel();\n      return modelID;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not retrieve current model from WindowAi.\");\n    }\n  }\n  async _ensureAiAvailable() {\n    let timeoutCounter = 0;\n    while (!this.globalContext.ai) {\n      await new Promise(resolve => setTimeout(resolve, 100));\n      timeoutCounter += 100;\n      if (timeoutCounter >= 1000) {\n        console.error(\"Please visit https://windowai.io to install WindowAi.\");\n        break;\n      }\n    }\n    if (this.globalContext.ai) {\n      console.log(\"WindowAi detected!\");\n    }\n  }\n}","map":{"version":3,"names":["LLM","WindowAi","constructor","fields","_fields$temperature","_fields$maxTokens","_fields$model","_fields$completionOpt","temperature","maxTokens","model","completionOptions","globalContext","window","globalThis","_ensureAiAvailable","_llmType","_call","prompt","stopSequences","input","messages","options","output","ai","getCompletion","text","error","console","log","Error","getCurrentModel","modelID","timeoutCounter","Promise","resolve","setTimeout"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/src/WindowAi.ts"],"sourcesContent":["import { BaseLLMParams } from \"langchain/dist/llms/base\";\nimport { LLM } from \"langchain/dist/llms/base\";\n\ninterface WindowAiInput extends BaseLLMParams {\n    temperature?: number;\n    maxTokens?: number;\n    model?: string;\n    completionOptions?: object;\n  }\n  \n  export class WindowAi extends LLM implements WindowAiInput {\n    temperature = 0;\n    maxTokens = 250;\n    model: string;\n    completionOptions: object;\n    globalContext: any;\n  \n    constructor(fields?: WindowAiInput) {\n      super(fields ?? {});\n  \n      this.temperature = fields?.temperature ?? this.temperature;\n      this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n      this.model = fields?.model ?? this.model;\n      this.completionOptions = fields?.completionOptions ?? {};\n  \n      this.globalContext = (typeof window !== \"undefined\") ? window : globalThis;\n  \n      this._ensureAiAvailable();\n    }\n  \n    _llmType(): string {\n      return \"windowai\";\n    }\n  \n    async _call(prompt: string, stopSequences?: string[]): Promise<string> {\n      const input = typeof prompt === \"string\" ? { prompt } : { messages: prompt };\n      const options = {\n        ...this.completionOptions,\n        temperature: this.temperature,\n        maxTokens: this.maxTokens,\n        model: this.model,\n        stopSequences,\n      };\n  \n      try {\n        const output = await this.globalContext.ai.getCompletion(input, options);\n        return output.text;\n      } catch (error) {\n        console.log(error);\n        throw new Error(\"Could not generate response from WindowAi.\");\n      }\n    }\n  \n    async getCurrentModel(): Promise<string> {\n      try {\n        const modelID = await this.globalContext.ai.getCurrentModel();\n        return modelID;\n      } catch (error) {\n        console.log(error);\n        throw new Error(\"Could not retrieve current model from WindowAi.\");\n      }\n    }\n  \n    private async _ensureAiAvailable(): Promise<void> {\n      let timeoutCounter = 0;\n      while (!this.globalContext.ai) {\n        await new Promise((resolve) => setTimeout(resolve, 100));\n        timeoutCounter += 100;\n        if (timeoutCounter >= 1000) {\n          console.error(\"Please visit https://windowai.io to install WindowAi.\");\n          break;\n        }\n      }\n  \n      if (this.globalContext.ai) {\n        console.log(\"WindowAi detected!\");\n      }\n    }\n  }\n  "],"mappings":"AACA,SAASA,GAAG,QAAQ,0BAA0B;AAS5C,OAAO,MAAMC,QAAQ,SAASD,GAAG,CAA0B;EAOzDE,WAAWA,CAACC,MAAsB,EAAE;IAAA,IAAAC,mBAAA,EAAAC,iBAAA,EAAAC,aAAA,EAAAC,qBAAA;IAClC,KAAK,CAACJ,MAAM,aAANA,MAAM,cAANA,MAAM,GAAI,CAAC,CAAC,CAAC;IAAC,KAPtBK,WAAW,GAAG,CAAC;IAAA,KACfC,SAAS,GAAG,GAAG;IAAA,KACfC,KAAK;IAAA,KACLC,iBAAiB;IAAA,KACjBC,aAAa;IAKX,IAAI,CAACJ,WAAW,IAAAJ,mBAAA,GAAGD,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEK,WAAW,cAAAJ,mBAAA,cAAAA,mBAAA,GAAI,IAAI,CAACI,WAAW;IAC1D,IAAI,CAACC,SAAS,IAAAJ,iBAAA,GAAGF,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEM,SAAS,cAAAJ,iBAAA,cAAAA,iBAAA,GAAI,IAAI,CAACI,SAAS;IACpD,IAAI,CAACC,KAAK,IAAAJ,aAAA,GAAGH,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEO,KAAK,cAAAJ,aAAA,cAAAA,aAAA,GAAI,IAAI,CAACI,KAAK;IACxC,IAAI,CAACC,iBAAiB,IAAAJ,qBAAA,GAAGJ,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEQ,iBAAiB,cAAAJ,qBAAA,cAAAA,qBAAA,GAAI,CAAC,CAAC;IAExD,IAAI,CAACK,aAAa,GAAI,OAAOC,MAAM,KAAK,WAAW,GAAIA,MAAM,GAAGC,UAAU;IAE1E,IAAI,CAACC,kBAAkB,EAAE;EAC3B;EAEAC,QAAQA,CAAA,EAAW;IACjB,OAAO,UAAU;EACnB;EAEA,MAAMC,KAAKA,CAACC,MAAc,EAAEC,aAAwB,EAAmB;IACrE,MAAMC,KAAK,GAAG,OAAOF,MAAM,KAAK,QAAQ,GAAG;MAAEA;IAAO,CAAC,GAAG;MAAEG,QAAQ,EAAEH;IAAO,CAAC;IAC5E,MAAMI,OAAO,GAAG;MACd,GAAG,IAAI,CAACX,iBAAiB;MACzBH,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BC,SAAS,EAAE,IAAI,CAACA,SAAS;MACzBC,KAAK,EAAE,IAAI,CAACA,KAAK;MACjBS;IACF,CAAC;IAED,IAAI;MACF,MAAMI,MAAM,GAAG,MAAM,IAAI,CAACX,aAAa,CAACY,EAAE,CAACC,aAAa,CAACL,KAAK,EAAEE,OAAO,CAAC;MACxE,OAAOC,MAAM,CAACG,IAAI;IACpB,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,4CAA4C,CAAC;IAC/D;EACF;EAEA,MAAMC,eAAeA,CAAA,EAAoB;IACvC,IAAI;MACF,MAAMC,OAAO,GAAG,MAAM,IAAI,CAACpB,aAAa,CAACY,EAAE,CAACO,eAAe,EAAE;MAC7D,OAAOC,OAAO;IAChB,CAAC,CAAC,OAAOL,KAAK,EAAE;MACdC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,iDAAiD,CAAC;IACpE;EACF;EAEA,MAAcf,kBAAkBA,CAAA,EAAkB;IAChD,IAAIkB,cAAc,GAAG,CAAC;IACtB,OAAO,CAAC,IAAI,CAACrB,aAAa,CAACY,EAAE,EAAE;MAC7B,MAAM,IAAIU,OAAO,CAAEC,OAAO,IAAKC,UAAU,CAACD,OAAO,EAAE,GAAG,CAAC,CAAC;MACxDF,cAAc,IAAI,GAAG;MACrB,IAAIA,cAAc,IAAI,IAAI,EAAE;QAC1BL,OAAO,CAACD,KAAK,CAAC,uDAAuD,CAAC;QACtE;MACF;IACF;IAEA,IAAI,IAAI,CAACf,aAAa,CAACY,EAAE,EAAE;MACzBI,OAAO,CAACC,GAAG,CAAC,oBAAoB,CAAC;IACnC;EACF;AACF"},"metadata":{},"sourceType":"module","externalDependencies":[]}
{"ast":null,"code":"// https://www.npmjs.com/package/@dqbd/tiktoken\nexport const getModelNameForTiktoken = modelName => {\n  if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n    return \"gpt-3.5-turbo\";\n  }\n  if (modelName.startsWith(\"gpt-4-32k-\")) {\n    return \"gpt-4-32k\";\n  }\n  if (modelName.startsWith(\"gpt-4-\")) {\n    return \"gpt-4\";\n  }\n  return modelName;\n};\nexport const getModelContextSize = modelName => {\n  switch (getModelNameForTiktoken(modelName)) {\n    case \"text-davinci-003\":\n      return 4097;\n    case \"text-curie-001\":\n      return 2048;\n    case \"text-babbage-001\":\n      return 2048;\n    case \"text-ada-001\":\n      return 2048;\n    case \"code-davinci-002\":\n      return 8000;\n    case \"code-cushman-001\":\n      return 2048;\n    default:\n      return 4097;\n  }\n};\nexport const importTiktoken = async () => {\n  try {\n    const {\n      encoding_for_model\n    } = await import(\"@dqbd/tiktoken\");\n    return {\n      encoding_for_model\n    };\n  } catch (error) {\n    console.log(error);\n    return {\n      encoding_for_model: null\n    };\n  }\n};\nexport const calculateMaxTokens = async _ref => {\n  let {\n    prompt,\n    modelName\n  } = _ref;\n  const {\n    encoding_for_model\n  } = await importTiktoken();\n  // fallback to approximate calculation if tiktoken is not available\n  let numTokens = Math.ceil(prompt.length / 4);\n  try {\n    if (encoding_for_model) {\n      const encoding = encoding_for_model(getModelNameForTiktoken(modelName));\n      const tokenized = encoding.encode(prompt);\n      numTokens = tokenized.length;\n      encoding.free();\n    }\n  } catch (error) {\n    console.warn(\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\", error);\n  }\n  const maxTokens = getModelContextSize(modelName);\n  return maxTokens - numTokens;\n};","map":{"version":3,"names":["getModelNameForTiktoken","modelName","startsWith","getModelContextSize","importTiktoken","encoding_for_model","error","console","log","calculateMaxTokens","_ref","prompt","numTokens","Math","ceil","length","encoding","tokenized","encode","free","warn","maxTokens"],"sources":["/Users/b/Code/langchainjs/langchain/dist/base_language/count_tokens.js"],"sourcesContent":["// https://www.npmjs.com/package/@dqbd/tiktoken\nexport const getModelNameForTiktoken = (modelName) => {\n    if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n        return \"gpt-3.5-turbo\";\n    }\n    if (modelName.startsWith(\"gpt-4-32k-\")) {\n        return \"gpt-4-32k\";\n    }\n    if (modelName.startsWith(\"gpt-4-\")) {\n        return \"gpt-4\";\n    }\n    return modelName;\n};\nexport const getModelContextSize = (modelName) => {\n    switch (getModelNameForTiktoken(modelName)) {\n        case \"text-davinci-003\":\n            return 4097;\n        case \"text-curie-001\":\n            return 2048;\n        case \"text-babbage-001\":\n            return 2048;\n        case \"text-ada-001\":\n            return 2048;\n        case \"code-davinci-002\":\n            return 8000;\n        case \"code-cushman-001\":\n            return 2048;\n        default:\n            return 4097;\n    }\n};\nexport const importTiktoken = async () => {\n    try {\n        const { encoding_for_model } = await import(\"@dqbd/tiktoken\");\n        return { encoding_for_model };\n    }\n    catch (error) {\n        console.log(error);\n        return { encoding_for_model: null };\n    }\n};\nexport const calculateMaxTokens = async ({ prompt, modelName, }) => {\n    const { encoding_for_model } = await importTiktoken();\n    // fallback to approximate calculation if tiktoken is not available\n    let numTokens = Math.ceil(prompt.length / 4);\n    try {\n        if (encoding_for_model) {\n            const encoding = encoding_for_model(getModelNameForTiktoken(modelName));\n            const tokenized = encoding.encode(prompt);\n            numTokens = tokenized.length;\n            encoding.free();\n        }\n    }\n    catch (error) {\n        console.warn(\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\", error);\n    }\n    const maxTokens = getModelContextSize(modelName);\n    return maxTokens - numTokens;\n};\n"],"mappings":"AAAA;AACA,OAAO,MAAMA,uBAAuB,GAAIC,SAAS,IAAK;EAClD,IAAIA,SAAS,CAACC,UAAU,CAAC,gBAAgB,CAAC,EAAE;IACxC,OAAO,eAAe;EAC1B;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,YAAY,CAAC,EAAE;IACpC,OAAO,WAAW;EACtB;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,QAAQ,CAAC,EAAE;IAChC,OAAO,OAAO;EAClB;EACA,OAAOD,SAAS;AACpB,CAAC;AACD,OAAO,MAAME,mBAAmB,GAAIF,SAAS,IAAK;EAC9C,QAAQD,uBAAuB,CAACC,SAAS,CAAC;IACtC,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,gBAAgB;MACjB,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,cAAc;MACf,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf;MACI,OAAO,IAAI;EAAC;AAExB,CAAC;AACD,OAAO,MAAMG,cAAc,GAAG,MAAAA,CAAA,KAAY;EACtC,IAAI;IACA,MAAM;MAAEC;IAAmB,CAAC,GAAG,MAAM,MAAM,CAAC,gBAAgB,CAAC;IAC7D,OAAO;MAAEA;IAAmB,CAAC;EACjC,CAAC,CACD,OAAOC,KAAK,EAAE;IACVC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;IAClB,OAAO;MAAED,kBAAkB,EAAE;IAAK,CAAC;EACvC;AACJ,CAAC;AACD,OAAO,MAAMI,kBAAkB,GAAG,MAAAC,IAAA,IAAkC;EAAA,IAA3B;IAAEC,MAAM;IAAEV;EAAW,CAAC,GAAAS,IAAA;EAC3D,MAAM;IAAEL;EAAmB,CAAC,GAAG,MAAMD,cAAc,EAAE;EACrD;EACA,IAAIQ,SAAS,GAAGC,IAAI,CAACC,IAAI,CAACH,MAAM,CAACI,MAAM,GAAG,CAAC,CAAC;EAC5C,IAAI;IACA,IAAIV,kBAAkB,EAAE;MACpB,MAAMW,QAAQ,GAAGX,kBAAkB,CAACL,uBAAuB,CAACC,SAAS,CAAC,CAAC;MACvE,MAAMgB,SAAS,GAAGD,QAAQ,CAACE,MAAM,CAACP,MAAM,CAAC;MACzCC,SAAS,GAAGK,SAAS,CAACF,MAAM;MAC5BC,QAAQ,CAACG,IAAI,EAAE;IACnB;EACJ,CAAC,CACD,OAAOb,KAAK,EAAE;IACVC,OAAO,CAACa,IAAI,CAAC,uFAAuF,EAAEd,KAAK,CAAC;EAChH;EACA,MAAMe,SAAS,GAAGlB,mBAAmB,CAACF,SAAS,CAAC;EAChD,OAAOoB,SAAS,GAAGT,SAAS;AAChC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}
{"ast":null,"code":"import { getCallbackManager } from \"../callbacks/index.js\";\nimport { AsyncCaller } from \"../util/async_caller.js\";\nimport { getModelNameForTiktoken, importTiktoken } from \"./count_tokens.js\";\nconst getVerbosity = () => false;\n/**\n * Base class for language models.\n */\nexport class BaseLanguageModel {\n  constructor(params) {\n    /**\n     * Whether to print out response text.\n     */\n    Object.defineProperty(this, \"verbose\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"callbackManager\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    /**\n     * The async caller should be used by subclasses to make any async calls,\n     * which will thus benefit from the concurrency and retry logic.\n     */\n    Object.defineProperty(this, \"caller\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"_encoding\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"_registry\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.verbose = params.verbose ?? (params.callbackManager ? true : getVerbosity());\n    this.callbackManager = params.callbackManager ?? getCallbackManager();\n    this.caller = new AsyncCaller(params ?? {});\n  }\n  async getNumTokens(text) {\n    // fallback to approximate calculation if tiktoken is not available\n    let numTokens = Math.ceil(text.length / 4);\n    try {\n      if (!this._encoding) {\n        const {\n          encoding_for_model\n        } = await importTiktoken();\n        // modelName only exists in openai subclasses, but tiktoken only supports\n        // openai tokenisers anyway, so for other subclasses we default to gpt2\n        if (encoding_for_model) {\n          this._encoding = encoding_for_model(\"modelName\" in this ? getModelNameForTiktoken(this.modelName) : \"gpt2\");\n          // We need to register a finalizer to free the tokenizer when the\n          // model is garbage collected.\n          this._registry = new FinalizationRegistry(t => t.free());\n          this._registry.register(this, this._encoding);\n        }\n      }\n      if (this._encoding) {\n        numTokens = this._encoding.encode(text).length;\n      }\n    } catch (error) {\n      console.warn(\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\", error);\n    }\n    return numTokens;\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  /**\n   * Load an LLM from a json-like object describing it.\n   */\n  static async deserialize(data) {\n    const {\n      _type,\n      _model,\n      ...rest\n    } = data;\n    if (_model && _model !== \"base_chat_model\") {\n      throw new Error(`Cannot load LLM with model ${_model}`);\n    }\n    const Cls = {\n      openai: (await import(\"../chat_models/openai.js\")).ChatOpenAI\n    }[_type];\n    if (Cls === undefined) {\n      throw new Error(`Cannot load  LLM with type ${_type}`);\n    }\n    return new Cls(rest);\n  }\n}","map":{"version":3,"names":["getCallbackManager","AsyncCaller","getModelNameForTiktoken","importTiktoken","getVerbosity","BaseLanguageModel","constructor","params","Object","defineProperty","enumerable","configurable","writable","value","verbose","callbackManager","caller","getNumTokens","text","numTokens","Math","ceil","length","_encoding","encoding_for_model","modelName","_registry","FinalizationRegistry","t","free","register","encode","error","console","warn","_identifyingParams","serialize","_type","_llmType","_model","_modelType","deserialize","data","rest","Error","Cls","openai","ChatOpenAI","undefined"],"sources":["/Users/b/Code/langchainjs/langchain/dist/base_language/index.js"],"sourcesContent":["import { getCallbackManager } from \"../callbacks/index.js\";\nimport { AsyncCaller } from \"../util/async_caller.js\";\nimport { getModelNameForTiktoken, importTiktoken } from \"./count_tokens.js\";\nconst getVerbosity = () => false;\n/**\n * Base class for language models.\n */\nexport class BaseLanguageModel {\n    constructor(params) {\n        /**\n         * Whether to print out response text.\n         */\n        Object.defineProperty(this, \"verbose\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"callbackManager\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        /**\n         * The async caller should be used by subclasses to make any async calls,\n         * which will thus benefit from the concurrency and retry logic.\n         */\n        Object.defineProperty(this, \"caller\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"_encoding\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"_registry\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.verbose =\n            params.verbose ?? (params.callbackManager ? true : getVerbosity());\n        this.callbackManager = params.callbackManager ?? getCallbackManager();\n        this.caller = new AsyncCaller(params ?? {});\n    }\n    async getNumTokens(text) {\n        // fallback to approximate calculation if tiktoken is not available\n        let numTokens = Math.ceil(text.length / 4);\n        try {\n            if (!this._encoding) {\n                const { encoding_for_model } = await importTiktoken();\n                // modelName only exists in openai subclasses, but tiktoken only supports\n                // openai tokenisers anyway, so for other subclasses we default to gpt2\n                if (encoding_for_model) {\n                    this._encoding = encoding_for_model(\"modelName\" in this\n                        ? getModelNameForTiktoken(this.modelName)\n                        : \"gpt2\");\n                    // We need to register a finalizer to free the tokenizer when the\n                    // model is garbage collected.\n                    this._registry = new FinalizationRegistry((t) => t.free());\n                    this._registry.register(this, this._encoding);\n                }\n            }\n            if (this._encoding) {\n                numTokens = this._encoding.encode(text).length;\n            }\n        }\n        catch (error) {\n            console.warn(\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\", error);\n        }\n        return numTokens;\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    /**\n     * Load an LLM from a json-like object describing it.\n     */\n    static async deserialize(data) {\n        const { _type, _model, ...rest } = data;\n        if (_model && _model !== \"base_chat_model\") {\n            throw new Error(`Cannot load LLM with model ${_model}`);\n        }\n        const Cls = {\n            openai: (await import(\"../chat_models/openai.js\")).ChatOpenAI,\n        }[_type];\n        if (Cls === undefined) {\n            throw new Error(`Cannot load  LLM with type ${_type}`);\n        }\n        return new Cls(rest);\n    }\n}\n"],"mappings":"AAAA,SAASA,kBAAkB,QAAQ,uBAAuB;AAC1D,SAASC,WAAW,QAAQ,yBAAyB;AACrD,SAASC,uBAAuB,EAAEC,cAAc,QAAQ,mBAAmB;AAC3E,MAAMC,YAAY,GAAGA,CAAA,KAAM,KAAK;AAChC;AACA;AACA;AACA,OAAO,MAAMC,iBAAiB,CAAC;EAC3BC,WAAWA,CAACC,MAAM,EAAE;IAChB;AACR;AACA;IACQC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF;AACR;AACA;AACA;IACQL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACC,OAAO,GACRP,MAAM,CAACO,OAAO,KAAKP,MAAM,CAACQ,eAAe,GAAG,IAAI,GAAGX,YAAY,EAAE,CAAC;IACtE,IAAI,CAACW,eAAe,GAAGR,MAAM,CAACQ,eAAe,IAAIf,kBAAkB,EAAE;IACrE,IAAI,CAACgB,MAAM,GAAG,IAAIf,WAAW,CAACM,MAAM,IAAI,CAAC,CAAC,CAAC;EAC/C;EACA,MAAMU,YAAYA,CAACC,IAAI,EAAE;IACrB;IACA,IAAIC,SAAS,GAAGC,IAAI,CAACC,IAAI,CAACH,IAAI,CAACI,MAAM,GAAG,CAAC,CAAC;IAC1C,IAAI;MACA,IAAI,CAAC,IAAI,CAACC,SAAS,EAAE;QACjB,MAAM;UAAEC;QAAmB,CAAC,GAAG,MAAMrB,cAAc,EAAE;QACrD;QACA;QACA,IAAIqB,kBAAkB,EAAE;UACpB,IAAI,CAACD,SAAS,GAAGC,kBAAkB,CAAC,WAAW,IAAI,IAAI,GACjDtB,uBAAuB,CAAC,IAAI,CAACuB,SAAS,CAAC,GACvC,MAAM,CAAC;UACb;UACA;UACA,IAAI,CAACC,SAAS,GAAG,IAAIC,oBAAoB,CAAEC,CAAC,IAAKA,CAAC,CAACC,IAAI,EAAE,CAAC;UAC1D,IAAI,CAACH,SAAS,CAACI,QAAQ,CAAC,IAAI,EAAE,IAAI,CAACP,SAAS,CAAC;QACjD;MACJ;MACA,IAAI,IAAI,CAACA,SAAS,EAAE;QAChBJ,SAAS,GAAG,IAAI,CAACI,SAAS,CAACQ,MAAM,CAACb,IAAI,CAAC,CAACI,MAAM;MAClD;IACJ,CAAC,CACD,OAAOU,KAAK,EAAE;MACVC,OAAO,CAACC,IAAI,CAAC,uFAAuF,EAAEF,KAAK,CAAC;IAChH;IACA,OAAOb,SAAS;EACpB;EACA;AACJ;AACA;EACI;EACAgB,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACD,kBAAkB,EAAE;MAC5BE,KAAK,EAAE,IAAI,CAACC,QAAQ,EAAE;MACtBC,MAAM,EAAE,IAAI,CAACC,UAAU;IAC3B,CAAC;EACL;EACA;AACJ;AACA;EACI,aAAaC,WAAWA,CAACC,IAAI,EAAE;IAC3B,MAAM;MAAEL,KAAK;MAAEE,MAAM;MAAE,GAAGI;IAAK,CAAC,GAAGD,IAAI;IACvC,IAAIH,MAAM,IAAIA,MAAM,KAAK,iBAAiB,EAAE;MACxC,MAAM,IAAIK,KAAK,CAAE,8BAA6BL,MAAO,EAAC,CAAC;IAC3D;IACA,MAAMM,GAAG,GAAG;MACRC,MAAM,EAAE,CAAC,MAAM,MAAM,CAAC,0BAA0B,CAAC,EAAEC;IACvD,CAAC,CAACV,KAAK,CAAC;IACR,IAAIQ,GAAG,KAAKG,SAAS,EAAE;MACnB,MAAM,IAAIJ,KAAK,CAAE,8BAA6BP,KAAM,EAAC,CAAC;IAC1D;IACA,OAAO,IAAIQ,GAAG,CAACF,IAAI,CAAC;EACxB;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
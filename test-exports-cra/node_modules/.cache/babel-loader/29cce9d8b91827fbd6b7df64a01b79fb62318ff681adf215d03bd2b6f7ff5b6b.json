{"ast":null,"code":"import { LLM } from \"langchain/llms/base.js\";\n\n// interface WindowAiInput extends BaseLLMParams {\n//     prompt: string;\n//     completionOptions?: CompletionOptions\n//   }\n//   Input is either a { prompt : string } or { messages: ChatMessage[]}\n\nexport let ModelID = /*#__PURE__*/function (ModelID) {\n  ModelID[\"GPT3\"] = \"openai/gpt3.5\";\n  ModelID[\"GPT4\"] = \"openai/gpt4\";\n  ModelID[\"GPTNeo\"] = \"together/gpt-neoxt-20B\";\n  ModelID[\"Cohere\"] = \"cohere/xlarge\";\n  ModelID[\"Local\"] = \"local\";\n  return ModelID;\n}({});\nexport class WindowAi extends LLM {\n  constructor(fields) {\n    var _fields$completionOpt;\n    super(fields !== null && fields !== void 0 ? fields : {});\n    this.temperature = 0;\n    this.maxTokens = 250;\n    this.model = void 0;\n    this.completionOptions = void 0;\n    this.globalContext = void 0;\n    this.completionOptions = (_fields$completionOpt = fields === null || fields === void 0 ? void 0 : fields.completionOptions) !== null && _fields$completionOpt !== void 0 ? _fields$completionOpt : {};\n    this.globalContext = typeof window !== \"undefined\" ? window : globalThis;\n    this._ensureAiAvailable();\n  }\n  _llmType() {\n    return \"windowai\";\n  }\n  async _call(prompt, _stop) {\n    const input = typeof prompt === \"string\" ? {\n      prompt\n    } : {\n      messages: prompt\n    };\n    const completionOptions = {\n      ...this.completionOptions\n    };\n    //completion options if not passed in are set to this.completionOptions\n\n    // const options = {\n    // ...this.completionOptions,\n    // temperature: this.temperature,\n    // maxTokens: this.maxTokens,\n    // model: this.model, //ModelID e.g. GPT3\n    // stopSequences,\n    // };\n\n    try {\n      const output = await this.globalContext.ai.getCompletion(input, completionOptions);\n      return output.text;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not generate response from WindowAi.\");\n    }\n  }\n  async getCurrentModel() {\n    try {\n      const modelID = await this.globalContext.ai.getCurrentModel();\n      return modelID;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not retrieve current model from WindowAi.\");\n    }\n  }\n  async _ensureAiAvailable() {\n    let timeoutCounter = 0;\n    while (!this.globalContext.ai) {\n      await new Promise(resolve => setTimeout(resolve, 100));\n      timeoutCounter += 100;\n      if (timeoutCounter >= 1000) {\n        console.error(\"Please visit https://windowai.io to install WindowAi.\");\n        break;\n      }\n    }\n    if (this.globalContext.ai) {\n      console.log(\"WindowAi detected!\");\n    }\n  }\n}","map":{"version":3,"names":["LLM","ModelID","WindowAi","constructor","fields","_fields$completionOpt","temperature","maxTokens","model","completionOptions","globalContext","window","globalThis","_ensureAiAvailable","_llmType","_call","prompt","_stop","input","messages","output","ai","getCompletion","text","error","console","log","Error","getCurrentModel","modelID","timeoutCounter","Promise","resolve","setTimeout"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/src/WindowAi.ts"],"sourcesContent":["\nimport { LLM, BaseLLMParams } from \"langchain/llms/base.js\";\n\n// interface WindowAiInput extends BaseLLMParams {\n//     prompt: string;\n//     completionOptions?: CompletionOptions\n//   }\n//   Input is either a { prompt : string } or { messages: ChatMessage[]}\n\nexport enum ModelID {\n    GPT3 = \"openai/gpt3.5\",\n    GPT4 = \"openai/gpt4\",\n    GPTNeo = \"together/gpt-neoxt-20B\",\n    Cohere = \"cohere/xlarge\",\n    Local = \"local\"\n  }\n\ninterface CompletionOptions {\n\n    // If specified, partial updates will be streamed to this handler as they become available,\n    // and only the first partial update will be returned by the Promise.\n    onStreamResult?: (result: Output | null, error: string | null) => unknown\n\n    // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n    // make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n    // Different models have different defaults.\n    temperature?: number\n\n    // How many completion choices to generate. Defaults to 1.\n    numOutputs?: number\n\n    // The maximum number of tokens to generate in the chat completion. Defaults to infinity, but the\n    // total length of input tokens and generated tokens is limited by the model's context length.\n    maxTokens?: number\n\n    // Sequences where the API will stop generating further tokens.\n    stopSequences?: string[]\n\n    // Identifier of the model to use. Defaults to the user's current model, but can be overridden here.\n    model?: ModelID \n}\n  \nexport type ChatMessage = {\n    role: \"system\" | \"user\" | \"assistant\"\n    content: string\n}\n  \n\nexport type Output = | { text: string } | { message: ChatMessage }\nexport type Input =  { prompt: string } | { messages: ChatMessage[] }\n\n\n\nexport class WindowAi extends LLM implements WindowAiInput {\n    temperature = 0;\n    maxTokens = 250;\n    model: string;\n    completionOptions: object;\n    globalContext: any;\n\n    constructor(fields?: WindowAiInput) {\n        super(fields ?? {});\n\n\n        this.completionOptions = fields?.completionOptions ?? {};\n\n        this.globalContext = (typeof window !== \"undefined\") ? window : globalThis;\n\n\n\n        this._ensureAiAvailable();\n    }\n\n    _llmType(): string {\n        return \"windowai\";\n    }\n\n    async _call(prompt: string, _stop): Promise<string> {\n        const input : Input = typeof prompt === \"string\" ? { prompt } : { messages: prompt };\n        const completionOptions : CompletionOptions = {\n            ...this.completionOptions,\n        }\n        //completion options if not passed in are set to this.completionOptions\n\n        // const options = {\n        // ...this.completionOptions,\n        // temperature: this.temperature,\n        // maxTokens: this.maxTokens,\n        // model: this.model, //ModelID e.g. GPT3\n        // stopSequences,\n        // };\n\n\n        try {\n         const output = await this.globalContext.ai.getCompletion(input, completionOptions);\n             return output.text;\n        } catch (error) {\n        console.log(error);\n        throw new Error(\"Could not generate response from WindowAi.\");\n        }\n    }\n\n    async getCurrentModel(): Promise<string> {\n        try {\n        const modelID = await this.globalContext.ai.getCurrentModel();\n        return modelID;\n        } catch (error) {\n        console.log(error);\n        throw new Error(\"Could not retrieve current model from WindowAi.\");\n        }\n    }\n\n    private async _ensureAiAvailable(): Promise<void> {\n        let timeoutCounter = 0;\n        while (!this.globalContext.ai) {\n        await new Promise((resolve) => setTimeout(resolve, 100));\n        timeoutCounter += 100;\n        if (timeoutCounter >= 1000) {\n            console.error(\"Please visit https://windowai.io to install WindowAi.\");\n            break;\n        }\n        }\n\n        if (this.globalContext.ai) {\n        console.log(\"WindowAi detected!\");\n        }\n    }\n}\n"],"mappings":"AACA,SAASA,GAAG,QAAuB,wBAAwB;;AAE3D;AACA;AACA;AACA;AACA;;AAEA,WAAYC,OAAO,0BAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAA,OAAPA,OAAO;AAAA;AA4CnB,OAAO,MAAMC,QAAQ,SAASF,GAAG,CAA0B;EAOvDG,WAAWA,CAACC,MAAsB,EAAE;IAAA,IAAAC,qBAAA;IAChC,KAAK,CAACD,MAAM,aAANA,MAAM,cAANA,MAAM,GAAI,CAAC,CAAC,CAAC;IAAC,KAPxBE,WAAW,GAAG,CAAC;IAAA,KACfC,SAAS,GAAG,GAAG;IAAA,KACfC,KAAK;IAAA,KACLC,iBAAiB;IAAA,KACjBC,aAAa;IAMT,IAAI,CAACD,iBAAiB,IAAAJ,qBAAA,GAAGD,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEK,iBAAiB,cAAAJ,qBAAA,cAAAA,qBAAA,GAAI,CAAC,CAAC;IAExD,IAAI,CAACK,aAAa,GAAI,OAAOC,MAAM,KAAK,WAAW,GAAIA,MAAM,GAAGC,UAAU;IAI1E,IAAI,CAACC,kBAAkB,EAAE;EAC7B;EAEAC,QAAQA,CAAA,EAAW;IACf,OAAO,UAAU;EACrB;EAEA,MAAMC,KAAKA,CAACC,MAAc,EAAEC,KAAK,EAAmB;IAChD,MAAMC,KAAa,GAAG,OAAOF,MAAM,KAAK,QAAQ,GAAG;MAAEA;IAAO,CAAC,GAAG;MAAEG,QAAQ,EAAEH;IAAO,CAAC;IACpF,MAAMP,iBAAqC,GAAG;MAC1C,GAAG,IAAI,CAACA;IACZ,CAAC;IACD;;IAEA;IACA;IACA;IACA;IACA;IACA;IACA;;IAGA,IAAI;MACH,MAAMW,MAAM,GAAG,MAAM,IAAI,CAACV,aAAa,CAACW,EAAE,CAACC,aAAa,CAACJ,KAAK,EAAET,iBAAiB,CAAC;MAC9E,OAAOW,MAAM,CAACG,IAAI;IACvB,CAAC,CAAC,OAAOC,KAAK,EAAE;MAChBC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,4CAA4C,CAAC;IAC7D;EACJ;EAEA,MAAMC,eAAeA,CAAA,EAAoB;IACrC,IAAI;MACJ,MAAMC,OAAO,GAAG,MAAM,IAAI,CAACnB,aAAa,CAACW,EAAE,CAACO,eAAe,EAAE;MAC7D,OAAOC,OAAO;IACd,CAAC,CAAC,OAAOL,KAAK,EAAE;MAChBC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,iDAAiD,CAAC;IAClE;EACJ;EAEA,MAAcd,kBAAkBA,CAAA,EAAkB;IAC9C,IAAIiB,cAAc,GAAG,CAAC;IACtB,OAAO,CAAC,IAAI,CAACpB,aAAa,CAACW,EAAE,EAAE;MAC/B,MAAM,IAAIU,OAAO,CAAEC,OAAO,IAAKC,UAAU,CAACD,OAAO,EAAE,GAAG,CAAC,CAAC;MACxDF,cAAc,IAAI,GAAG;MACrB,IAAIA,cAAc,IAAI,IAAI,EAAE;QACxBL,OAAO,CAACD,KAAK,CAAC,uDAAuD,CAAC;QACtE;MACJ;IACA;IAEA,IAAI,IAAI,CAACd,aAAa,CAACW,EAAE,EAAE;MAC3BI,OAAO,CAACC,GAAG,CAAC,oBAAoB,CAAC;IACjC;EACJ;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
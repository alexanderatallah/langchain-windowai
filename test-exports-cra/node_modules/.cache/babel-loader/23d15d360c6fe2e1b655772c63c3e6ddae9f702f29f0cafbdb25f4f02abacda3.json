{"ast":null,"code":"import { BaseChain } from \"./base.js\";\nimport { BufferMemory } from \"../memory/buffer_memory.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\n/**\n * Chain to run queries against LLMs.\n * @augments BaseChain\n * @augments LLMChainInput\n *\n * @example\n * ```ts\n * import { LLMChain } from \"langchain/chains\";\n * import { OpenAI } from \"langchain/llms/openai\";\n * import { PromptTemplate } from \"langchain/prompts\";\n * const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = LLMChain({ llm: new OpenAI(), prompt });\n * ```\n */\nexport class LLMChain extends BaseChain {\n  get inputKeys() {\n    return this.prompt.inputVariables;\n  }\n  constructor(fields) {\n    super(fields.memory, fields.verbose, fields.callbackManager);\n    Object.defineProperty(this, \"prompt\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"llm\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"outputKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"text\"\n    });\n    Object.defineProperty(this, \"outputParser\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.prompt = fields.prompt;\n    this.llm = fields.llm;\n    this.outputKey = fields.outputKey ?? this.outputKey;\n    this.outputParser = fields.outputParser ?? this.outputParser;\n    if (this.prompt.outputParser) {\n      if (this.outputParser) {\n        throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n      }\n      this.outputParser = this.prompt.outputParser;\n    }\n  }\n  async _getFinalOutput(generations, promptValue) {\n    const completion = generations[0].text;\n    let finalCompletion;\n    if (this.outputParser) {\n      finalCompletion = await this.outputParser.parseWithPrompt(completion, promptValue);\n    } else {\n      finalCompletion = completion;\n    }\n    return finalCompletion;\n  }\n  async _call(values) {\n    let stop;\n    if (\"stop\" in values && Array.isArray(values.stop)) {\n      stop = values.stop;\n    }\n    const promptValue = await this.prompt.formatPromptValue(values);\n    const {\n      generations\n    } = await this.llm.generatePrompt([promptValue], stop);\n    return {\n      [this.outputKey]: await this._getFinalOutput(generations[0], promptValue)\n    };\n  }\n  /**\n   * Format prompt with values and pass to LLM\n   *\n   * @param values - keys to pass to prompt template\n   * @returns Completion from LLM.\n   *\n   * @example\n   * ```ts\n   * llm.predict({ adjective: \"funny\" })\n   * ```\n   */\n  async predict(values) {\n    const output = await this.call(values);\n    return output[this.outputKey];\n  }\n  _chainType() {\n    return \"llm_chain\";\n  }\n  static async deserialize(data) {\n    const {\n      llm,\n      prompt\n    } = data;\n    if (!llm) {\n      throw new Error(\"LLMChain must have llm\");\n    }\n    if (!prompt) {\n      throw new Error(\"LLMChain must have prompt\");\n    }\n    return new LLMChain({\n      llm: await BaseLanguageModel.deserialize(llm),\n      prompt: await BasePromptTemplate.deserialize(prompt)\n    });\n  }\n  serialize() {\n    return {\n      _type: this._chainType(),\n      llm: this.llm.serialize(),\n      prompt: this.prompt.serialize()\n    };\n  }\n}\n// eslint-disable-next-line max-len\nconst defaultTemplate = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:`;\nexport class ConversationChain extends LLMChain {\n  constructor(fields) {\n    super({\n      prompt: fields.prompt ?? new PromptTemplate({\n        template: defaultTemplate,\n        inputVariables: [\"history\", \"input\"]\n      }),\n      llm: fields.llm,\n      outputKey: fields.outputKey ?? \"response\"\n    });\n    this.memory = fields.memory ?? new BufferMemory();\n  }\n}","map":{"version":3,"names":["BaseChain","BufferMemory","PromptTemplate","BasePromptTemplate","BaseLanguageModel","LLMChain","inputKeys","prompt","inputVariables","constructor","fields","memory","verbose","callbackManager","Object","defineProperty","enumerable","configurable","writable","value","llm","outputKey","outputParser","Error","_getFinalOutput","generations","promptValue","completion","text","finalCompletion","parseWithPrompt","_call","values","stop","Array","isArray","formatPromptValue","generatePrompt","predict","output","call","_chainType","deserialize","data","serialize","_type","defaultTemplate","ConversationChain","template"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/node_modules/langchain/dist/chains/llm_chain.js"],"sourcesContent":["import { BaseChain } from \"./base.js\";\nimport { BufferMemory } from \"../memory/buffer_memory.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\n/**\n * Chain to run queries against LLMs.\n * @augments BaseChain\n * @augments LLMChainInput\n *\n * @example\n * ```ts\n * import { LLMChain } from \"langchain/chains\";\n * import { OpenAI } from \"langchain/llms/openai\";\n * import { PromptTemplate } from \"langchain/prompts\";\n * const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = LLMChain({ llm: new OpenAI(), prompt });\n * ```\n */\nexport class LLMChain extends BaseChain {\n    get inputKeys() {\n        return this.prompt.inputVariables;\n    }\n    constructor(fields) {\n        super(fields.memory, fields.verbose, fields.callbackManager);\n        Object.defineProperty(this, \"prompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llm\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text\"\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.prompt = fields.prompt;\n        this.llm = fields.llm;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.outputParser = fields.outputParser ?? this.outputParser;\n        if (this.prompt.outputParser) {\n            if (this.outputParser) {\n                throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n            }\n            this.outputParser = this.prompt.outputParser;\n        }\n    }\n    async _getFinalOutput(generations, promptValue) {\n        const completion = generations[0].text;\n        let finalCompletion;\n        if (this.outputParser) {\n            finalCompletion = await this.outputParser.parseWithPrompt(completion, promptValue);\n        }\n        else {\n            finalCompletion = completion;\n        }\n        return finalCompletion;\n    }\n    async _call(values) {\n        let stop;\n        if (\"stop\" in values && Array.isArray(values.stop)) {\n            stop = values.stop;\n        }\n        const promptValue = await this.prompt.formatPromptValue(values);\n        const { generations } = await this.llm.generatePrompt([promptValue], stop);\n        return {\n            [this.outputKey]: await this._getFinalOutput(generations[0], promptValue),\n        };\n    }\n    /**\n     * Format prompt with values and pass to LLM\n     *\n     * @param values - keys to pass to prompt template\n     * @returns Completion from LLM.\n     *\n     * @example\n     * ```ts\n     * llm.predict({ adjective: \"funny\" })\n     * ```\n     */\n    async predict(values) {\n        const output = await this.call(values);\n        return output[this.outputKey];\n    }\n    _chainType() {\n        return \"llm_chain\";\n    }\n    static async deserialize(data) {\n        const { llm, prompt } = data;\n        if (!llm) {\n            throw new Error(\"LLMChain must have llm\");\n        }\n        if (!prompt) {\n            throw new Error(\"LLMChain must have prompt\");\n        }\n        return new LLMChain({\n            llm: await BaseLanguageModel.deserialize(llm),\n            prompt: await BasePromptTemplate.deserialize(prompt),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm: this.llm.serialize(),\n            prompt: this.prompt.serialize(),\n        };\n    }\n}\n// eslint-disable-next-line max-len\nconst defaultTemplate = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:`;\nexport class ConversationChain extends LLMChain {\n    constructor(fields) {\n        super({\n            prompt: fields.prompt ??\n                new PromptTemplate({\n                    template: defaultTemplate,\n                    inputVariables: [\"history\", \"input\"],\n                }),\n            llm: fields.llm,\n            outputKey: fields.outputKey ?? \"response\",\n        });\n        this.memory = fields.memory ?? new BufferMemory();\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,QAAQ,WAAW;AACrC,SAASC,YAAY,QAAQ,4BAA4B;AACzD,SAASC,cAAc,QAAQ,sBAAsB;AACrD,SAASC,kBAAkB,QAAQ,oBAAoB;AACvD,SAASC,iBAAiB,QAAQ,2BAA2B;AAC7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,QAAQ,SAASL,SAAS,CAAC;EACpC,IAAIM,SAASA,CAAA,EAAG;IACZ,OAAO,IAAI,CAACC,MAAM,CAACC,cAAc;EACrC;EACAC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAACC,MAAM,EAAED,MAAM,CAACE,OAAO,EAAEF,MAAM,CAACG,eAAe,CAAC;IAC5DC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,KAAK,EAAE;MAC/BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACZ,MAAM,GAAGG,MAAM,CAACH,MAAM;IAC3B,IAAI,CAACa,GAAG,GAAGV,MAAM,CAACU,GAAG;IACrB,IAAI,CAACC,SAAS,GAAGX,MAAM,CAACW,SAAS,IAAI,IAAI,CAACA,SAAS;IACnD,IAAI,CAACC,YAAY,GAAGZ,MAAM,CAACY,YAAY,IAAI,IAAI,CAACA,YAAY;IAC5D,IAAI,IAAI,CAACf,MAAM,CAACe,YAAY,EAAE;MAC1B,IAAI,IAAI,CAACA,YAAY,EAAE;QACnB,MAAM,IAAIC,KAAK,CAAC,sDAAsD,CAAC;MAC3E;MACA,IAAI,CAACD,YAAY,GAAG,IAAI,CAACf,MAAM,CAACe,YAAY;IAChD;EACJ;EACA,MAAME,eAAeA,CAACC,WAAW,EAAEC,WAAW,EAAE;IAC5C,MAAMC,UAAU,GAAGF,WAAW,CAAC,CAAC,CAAC,CAACG,IAAI;IACtC,IAAIC,eAAe;IACnB,IAAI,IAAI,CAACP,YAAY,EAAE;MACnBO,eAAe,GAAG,MAAM,IAAI,CAACP,YAAY,CAACQ,eAAe,CAACH,UAAU,EAAED,WAAW,CAAC;IACtF,CAAC,MACI;MACDG,eAAe,GAAGF,UAAU;IAChC;IACA,OAAOE,eAAe;EAC1B;EACA,MAAME,KAAKA,CAACC,MAAM,EAAE;IAChB,IAAIC,IAAI;IACR,IAAI,MAAM,IAAID,MAAM,IAAIE,KAAK,CAACC,OAAO,CAACH,MAAM,CAACC,IAAI,CAAC,EAAE;MAChDA,IAAI,GAAGD,MAAM,CAACC,IAAI;IACtB;IACA,MAAMP,WAAW,GAAG,MAAM,IAAI,CAACnB,MAAM,CAAC6B,iBAAiB,CAACJ,MAAM,CAAC;IAC/D,MAAM;MAAEP;IAAY,CAAC,GAAG,MAAM,IAAI,CAACL,GAAG,CAACiB,cAAc,CAAC,CAACX,WAAW,CAAC,EAAEO,IAAI,CAAC;IAC1E,OAAO;MACH,CAAC,IAAI,CAACZ,SAAS,GAAG,MAAM,IAAI,CAACG,eAAe,CAACC,WAAW,CAAC,CAAC,CAAC,EAAEC,WAAW;IAC5E,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMY,OAAOA,CAACN,MAAM,EAAE;IAClB,MAAMO,MAAM,GAAG,MAAM,IAAI,CAACC,IAAI,CAACR,MAAM,CAAC;IACtC,OAAOO,MAAM,CAAC,IAAI,CAAClB,SAAS,CAAC;EACjC;EACAoB,UAAUA,CAAA,EAAG;IACT,OAAO,WAAW;EACtB;EACA,aAAaC,WAAWA,CAACC,IAAI,EAAE;IAC3B,MAAM;MAAEvB,GAAG;MAAEb;IAAO,CAAC,GAAGoC,IAAI;IAC5B,IAAI,CAACvB,GAAG,EAAE;MACN,MAAM,IAAIG,KAAK,CAAC,wBAAwB,CAAC;IAC7C;IACA,IAAI,CAAChB,MAAM,EAAE;MACT,MAAM,IAAIgB,KAAK,CAAC,2BAA2B,CAAC;IAChD;IACA,OAAO,IAAIlB,QAAQ,CAAC;MAChBe,GAAG,EAAE,MAAMhB,iBAAiB,CAACsC,WAAW,CAACtB,GAAG,CAAC;MAC7Cb,MAAM,EAAE,MAAMJ,kBAAkB,CAACuC,WAAW,CAACnC,MAAM;IACvD,CAAC,CAAC;EACN;EACAqC,SAASA,CAAA,EAAG;IACR,OAAO;MACHC,KAAK,EAAE,IAAI,CAACJ,UAAU,EAAE;MACxBrB,GAAG,EAAE,IAAI,CAACA,GAAG,CAACwB,SAAS,EAAE;MACzBrC,MAAM,EAAE,IAAI,CAACA,MAAM,CAACqC,SAAS;IACjC,CAAC;EACL;AACJ;AACA;AACA,MAAME,eAAe,GAAI;AACzB;AACA;AACA;AACA;AACA,IAAI;AACJ,OAAO,MAAMC,iBAAiB,SAAS1C,QAAQ,CAAC;EAC5CI,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC;MACFH,MAAM,EAAEG,MAAM,CAACH,MAAM,IACjB,IAAIL,cAAc,CAAC;QACf8C,QAAQ,EAAEF,eAAe;QACzBtC,cAAc,EAAE,CAAC,SAAS,EAAE,OAAO;MACvC,CAAC,CAAC;MACNY,GAAG,EAAEV,MAAM,CAACU,GAAG;MACfC,SAAS,EAAEX,MAAM,CAACW,SAAS,IAAI;IACnC,CAAC,CAAC;IACF,IAAI,CAACV,MAAM,GAAGD,MAAM,CAACC,MAAM,IAAI,IAAIV,YAAY,EAAE;EACrD;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
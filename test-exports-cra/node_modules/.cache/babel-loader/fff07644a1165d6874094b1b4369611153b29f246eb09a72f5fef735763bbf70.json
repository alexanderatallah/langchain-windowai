{"ast":null,"code":"var _jsxFileName = \"/Users/b/Code/langchainjs/test-exports-cra/src/App.js\",\n  _s = $RefreshSig$();\n/* eslint-disable no-unused-vars */\n\n// import all entrypoints to test, do not do this in your own app\nimport \"./entrypoints.js\";\n\n// Import a few things we'll use to test the exports\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate } from \"langchain/prompts\";\nimport { useCallback, useEffect, useState } from \"react\";\nimport { CallbackManager } from \"langchain/callbacks\";\nimport { WindowAi } from \"./WindowAi.ts\";\nimport { ModelID } from \"./WindowAi.ts\";\n\n// async function getResponse(prompt) {\n//   const response = await windowAi.call(prompt);\n//   console.log(response);\n// }\n\n// // Example usage\n// getResponse(\"Hello, WindowAi!\");\n\n// Don't do this in your app, it would leak your API key\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst OPENAI_API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nfunction App() {\n  _s();\n  // llm_res = useState()\n  const [llm_responses, setLlm_responses] = useState([]);\n  // turn llm_res into an array\n  // llm_res = [llm_res, setLlm_res] = useState();\n\n  const runChain = useCallback(async () => {\n    // WindowAi._ensureAiAvailable()\n\n    const llm = new WindowAi({\n      completionOptions: {\n        temperature: 0.7,\n        maxTokens: 800,\n        model: ModelID.GPT3\n      }\n    });\n    const template = `Question: {question}.  Answer: Let's think step by step.`;\n    const prompt = new PromptTemplate({\n      template: template,\n      inputVariables: [\"question\"]\n    });\n    const llm_chain = new LLMChain({\n      prompt: prompt,\n      llm: llm\n    });\n    const question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\";\n    const response = await llm_chain.run(question);\n\n    //   const llm = new WindowAi({\n    //     completionOptions : { temperature: 0.7, maxTokens: 800, model: ModelID.GPT3 }\n    //   });\n    //  const model = await llm.getCurrentModel()\n    //  console.log(\"model in use:\", model);\n    //   const response = await llm._call(\"Hello, WindowAi!\");\n\n    //     var template = `Question: {question}\n\n    //     Answer: Let's think step by step.`\n\n    //   var prompt = new PromptTemplate({ template:template, inputVariables:[\"question\"]})\n    //   var llm_chain = new LLMChain({ prompt:prompt, llm:llm})\n    //   var question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n    //   var r = await llm_chain.run(question)\n\n    //   setLlm_responses(llm_responses => [...llm_responses, r]);\n\n    //  const response = await llm._call(\"Hello, WindowAi!\");\n    //add new resonse to llm_res\n    //  setLlm_responses(llm_responses => [...llm_responses, response]);\n    //  console.log(\"response:\", response);\n    //  var llm_chain = LLMChain(prompt=prompt, llm=llm)\n    //  var response2 = await llm_chain.run(\"whats the capital of france?\");\n    //  setLlm_responses(llm_responses => [...llm_responses, response2]);\n\n    // const chain = new LLMChain({\n    //   llm,\n    //   prompt: \"hey\"\n    // // });\n    // const n = await llm.getNumTokens(\"Hello\");\n    // console.log(\"getNumTokens\", n);\n\n    // const chain = new LLMChain({\n    //   llm,\n    //   prompt: ChatPromptTemplate.fromPromptMessages([\n    //     HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n    //   ]),\n    // });\n\n    // const response = await windowAi._call({prompt: \"Hello, WindowAi!\"});\n\n    // const llm = new ChatOpenAI({\n    //   openAIApiKey: OPENAI_API_KEY,\n    //   streaming: true,\n    //   callbackManager: CallbackManager.fromHandlers({\n    //     handleLLMNewToken: async (token) =>\n    //       console.log(\"handleLLMNewToken\", token),\n    //   }),\n    // });\n\n    // // Test count tokens\n    // const n = await llm.getNumTokens(\"Hello\");\n\n    // // Test a chain + prompt + model\n    // const chain = new LLMChain({\n    //   llm,\n    //   prompt: ChatPromptTemplate.fromPromptMessages([\n    //     HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n    //   ]),\n    // });\n    // const res = await chain.run(\"hello\");\n\n    // console.log(\"runChain\", res);\n  }, []);\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"App\",\n    children: /*#__PURE__*/_jsxDEV(\"header\", {\n      className: \"App-header\",\n      children: [/*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"Edit \", /*#__PURE__*/_jsxDEV(\"code\", {\n          children: \"src/App.js\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 140,\n          columnNumber: 16\n        }, this), \" and save to reload.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 139,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n        className: \"App-link\",\n        href: \"https://reactjs.org\",\n        target: \"_blank\",\n        rel: \"noopener noreferrer\",\n        children: \"Learn React\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 142,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"button\", {\n        onClick: runChain,\n        children: \"Click to run a chain\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 150,\n        columnNumber: 9\n      }, this), llm_responses.map(response => /*#__PURE__*/_jsxDEV(\"p\", {\n        children: response\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 153,\n        columnNumber: 11\n      }, this))]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 138,\n      columnNumber: 7\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 137,\n    columnNumber: 5\n  }, this);\n}\n_s(App, \"xsVA1DW7v5PO+Qj2sogffmviKGs=\");\n_c = App;\nexport default App;\nvar _c;\n$RefreshReg$(_c, \"App\");","map":{"version":3,"names":["LLMChain","ChatOpenAI","ChatPromptTemplate","HumanMessagePromptTemplate","PromptTemplate","useCallback","useEffect","useState","CallbackManager","WindowAi","ModelID","jsxDEV","_jsxDEV","OPENAI_API_KEY","process","env","REACT_APP_OPENAI_API_KEY","App","_s","llm_responses","setLlm_responses","runChain","llm","completionOptions","temperature","maxTokens","model","GPT3","template","prompt","inputVariables","llm_chain","question","response","run","className","children","fileName","_jsxFileName","lineNumber","columnNumber","href","target","rel","onClick","map","_c","$RefreshReg$"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/src/App.js"],"sourcesContent":["/* eslint-disable no-unused-vars */\n\n// import all entrypoints to test, do not do this in your own app\nimport \"./entrypoints.js\";\n\n// Import a few things we'll use to test the exports\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate\n} from \"langchain/prompts\";\n\n\nimport { useCallback, useEffect, useState } from \"react\";\nimport { CallbackManager } from \"langchain/callbacks\";\n\nimport { WindowAi } from \"./WindowAi.ts\"\nimport { ModelID} from \"./WindowAi.ts\";\n\n\n// async function getResponse(prompt) {\n//   const response = await windowAi.call(prompt);\n//   console.log(response);\n// }\n\n// // Example usage\n// getResponse(\"Hello, WindowAi!\");\n\n\n// Don't do this in your app, it would leak your API key\nconst OPENAI_API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\n\nfunction App() {\n\n  // llm_res = useState()\n  const [llm_responses, setLlm_responses] = useState([]);\n  // turn llm_res into an array\n  // llm_res = [llm_res, setLlm_res] = useState();\n\n  const runChain = useCallback(async () => {\n\n        // WindowAi._ensureAiAvailable()\n\n        const llm = new WindowAi({  completionOptions : { temperature: 0.7, maxTokens: 800, model: ModelID.GPT3 } });\n        const template = `Question: {question}.  Answer: Let's think step by step.`\n        const prompt = new PromptTemplate({ template:template, inputVariables:[\"question\"]})\n        const llm_chain = new LLMChain({ prompt:prompt, llm:llm})\n        const question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n        const response = await llm_chain.run(question)\n\n\n    //   const llm = new WindowAi({\n    //     completionOptions : { temperature: 0.7, maxTokens: 800, model: ModelID.GPT3 }\n    //   });\n    //  const model = await llm.getCurrentModel()\n    //  console.log(\"model in use:\", model);\n    //   const response = await llm._call(\"Hello, WindowAi!\");\n\n    //     var template = `Question: {question}\n\n    //     Answer: Let's think step by step.`\n\n    //   var prompt = new PromptTemplate({ template:template, inputVariables:[\"question\"]})\n    //   var llm_chain = new LLMChain({ prompt:prompt, llm:llm})\n    //   var question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n    //   var r = await llm_chain.run(question)\n\n    //   setLlm_responses(llm_responses => [...llm_responses, r]);\n\n\n\n\n\n\n\n\n\n    //  const response = await llm._call(\"Hello, WindowAi!\");\n     //add new resonse to llm_res\n    //  setLlm_responses(llm_responses => [...llm_responses, response]);\n    //  console.log(\"response:\", response);\n    //  var llm_chain = LLMChain(prompt=prompt, llm=llm)\n    //  var response2 = await llm_chain.run(\"whats the capital of france?\");\n    //  setLlm_responses(llm_responses => [...llm_responses, response2]);\n\n    // const chain = new LLMChain({\n    //   llm,\n    //   prompt: \"hey\"\n    // // });\n    // const n = await llm.getNumTokens(\"Hello\");\n    // console.log(\"getNumTokens\", n);\n\n    // const chain = new LLMChain({\n    //   llm,\n    //   prompt: ChatPromptTemplate.fromPromptMessages([\n    //     HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n    //   ]),\n    // });\n\n\n\n      // const response = await windowAi._call({prompt: \"Hello, WindowAi!\"});\n     \n    \n    // const llm = new ChatOpenAI({\n    //   openAIApiKey: OPENAI_API_KEY,\n    //   streaming: true,\n    //   callbackManager: CallbackManager.fromHandlers({\n    //     handleLLMNewToken: async (token) =>\n    //       console.log(\"handleLLMNewToken\", token),\n    //   }),\n    // });\n\n    // // Test count tokens\n    // const n = await llm.getNumTokens(\"Hello\");\n\n\n\n\n    // // Test a chain + prompt + model\n    // const chain = new LLMChain({\n    //   llm,\n    //   prompt: ChatPromptTemplate.fromPromptMessages([\n    //     HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n    //   ]),\n    // });\n    // const res = await chain.run(\"hello\");\n\n    // console.log(\"runChain\", res);\n\n\n  }, []);\n\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p>\n          Edit <code>src/App.js</code> and save to reload.\n        </p>\n        <a\n          className=\"App-link\"\n          href=\"https://reactjs.org\"\n          target=\"_blank\"\n          rel=\"noopener noreferrer\"\n        >\n          Learn React\n        </a>\n        <button onClick={runChain}>Click to run a chain</button>\n\n        {llm_responses.map((response) => (\n          <p>{response}</p>\n        ))}\n\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n"],"mappings":";;AAAA;;AAEA;AACA,OAAO,kBAAkB;;AAEzB;AACA,SAASA,QAAQ,QAAQ,kBAAkB;AAC3C,SAASC,UAAU,QAAQ,8BAA8B;AACzD,SACEC,kBAAkB,EAClBC,0BAA0B,EAC1BC,cAAc,QACT,mBAAmB;AAG1B,SAASC,WAAW,EAAEC,SAAS,EAAEC,QAAQ,QAAQ,OAAO;AACxD,SAASC,eAAe,QAAQ,qBAAqB;AAErD,SAASC,QAAQ,QAAQ,eAAe;AACxC,SAASC,OAAO,QAAO,eAAe;;AAGtC;AACA;AACA;AACA;;AAEA;AACA;;AAGA;AAAA,SAAAC,MAAA,IAAAC,OAAA;AACA,MAAMC,cAAc,GAAGC,OAAO,CAACC,GAAG,CAACC,wBAAwB;AAE3D,SAASC,GAAGA,CAAA,EAAG;EAAAC,EAAA;EAEb;EACA,MAAM,CAACC,aAAa,EAAEC,gBAAgB,CAAC,GAAGb,QAAQ,CAAC,EAAE,CAAC;EACtD;EACA;;EAEA,MAAMc,QAAQ,GAAGhB,WAAW,CAAC,YAAY;IAEnC;;IAEA,MAAMiB,GAAG,GAAG,IAAIb,QAAQ,CAAC;MAAGc,iBAAiB,EAAG;QAAEC,WAAW,EAAE,GAAG;QAAEC,SAAS,EAAE,GAAG;QAAEC,KAAK,EAAEhB,OAAO,CAACiB;MAAK;IAAE,CAAC,CAAC;IAC5G,MAAMC,QAAQ,GAAI,0DAAyD;IAC3E,MAAMC,MAAM,GAAG,IAAIzB,cAAc,CAAC;MAAEwB,QAAQ,EAACA,QAAQ;MAAEE,cAAc,EAAC,CAAC,UAAU;IAAC,CAAC,CAAC;IACpF,MAAMC,SAAS,GAAG,IAAI/B,QAAQ,CAAC;MAAE6B,MAAM,EAACA,MAAM;MAAEP,GAAG,EAACA;IAAG,CAAC,CAAC;IACzD,MAAMU,QAAQ,GAAG,sEAAsE;IACvF,MAAMC,QAAQ,GAAG,MAAMF,SAAS,CAACG,GAAG,CAACF,QAAQ,CAAC;;IAGlD;IACA;IACA;IACA;IACA;IACA;;IAEA;;IAEA;;IAEA;IACA;IACA;IACA;;IAEA;;IAUA;IACC;IACD;IACA;IACA;IACA;IACA;;IAEA;IACA;IACA;IACA;IACA;IACA;;IAEA;IACA;IACA;IACA;IACA;IACA;;IAIE;;IAGF;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IAEA;IACA;;IAKA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IAEA;EAGF,CAAC,EAAE,EAAE,CAAC;EAEN,oBACEpB,OAAA;IAAKuB,SAAS,EAAC,KAAK;IAAAC,QAAA,eAClBxB,OAAA;MAAQuB,SAAS,EAAC,YAAY;MAAAC,QAAA,gBAC5BxB,OAAA;QAAAwB,QAAA,GAAG,OACI,eAAAxB,OAAA;UAAAwB,QAAA,EAAM;QAAU;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,QAAO,wBAC9B;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,QAAI,eACJ5B,OAAA;QACEuB,SAAS,EAAC,UAAU;QACpBM,IAAI,EAAC,qBAAqB;QAC1BC,MAAM,EAAC,QAAQ;QACfC,GAAG,EAAC,qBAAqB;QAAAP,QAAA,EAC1B;MAED;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,QAAI,eACJ5B,OAAA;QAAQgC,OAAO,EAAEvB,QAAS;QAAAe,QAAA,EAAC;MAAoB;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,QAAS,EAEvDrB,aAAa,CAAC0B,GAAG,CAAEZ,QAAQ,iBAC1BrB,OAAA;QAAAwB,QAAA,EAAIH;MAAQ;QAAAI,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,QACb,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA;EAEK;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,QACL;AAEV;AAACtB,EAAA,CA5HQD,GAAG;AAAA6B,EAAA,GAAH7B,GAAG;AA8HZ,eAAeA,GAAG;AAAC,IAAA6B,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}
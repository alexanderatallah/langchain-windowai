{"ast":null,"code":"import { LLM } from \"langchain/llms/base.js\";\nexport let ModelID = /*#__PURE__*/function (ModelID) {\n  ModelID[\"GPT3\"] = \"openai/gpt3.5\";\n  ModelID[\"GPT4\"] = \"openai/gpt4\";\n  ModelID[\"GPTNeo\"] = \"together/gpt-neoxt-20B\";\n  ModelID[\"Cohere\"] = \"cohere/xlarge\";\n  ModelID[\"Local\"] = \"local\";\n  return ModelID;\n}({});\n// export declare class WindowAi extends LLM implements WindowAiInput {\n//   temperature: number;\n//   maxTokens: number;\n//   model: string;\n//   completionOptions: CompletionOptions;\n\n//   constructor(fields?: WindowAiInput);\n\n//   _llmType(): string;\n\n//   _call(prompt: string | ChatMessage[], _stop?: string[]): Promise<string>;\n\n//   getCurrentModel(): Promise<string>;\n\n//   _ensureAiAvailable(): Promise<void>;\n// }\nexport class WindowAi extends LLM {\n  constructor(fields) {\n    var _fields$temperature, _fields$maxTokens, _fields$model, _fields$completionOpt;\n    super(fields !== null && fields !== void 0 ? fields : {});\n    this.temperature = 0;\n    this.maxTokens = 250;\n    this.model = void 0;\n    this.completionOptions = void 0;\n    this.globalContext = void 0;\n    this.temperature = (_fields$temperature = fields === null || fields === void 0 ? void 0 : fields.temperature) !== null && _fields$temperature !== void 0 ? _fields$temperature : this.temperature;\n    this.maxTokens = (_fields$maxTokens = fields === null || fields === void 0 ? void 0 : fields.maxTokens) !== null && _fields$maxTokens !== void 0 ? _fields$maxTokens : this.maxTokens;\n    this.model = (_fields$model = fields === null || fields === void 0 ? void 0 : fields.model) !== null && _fields$model !== void 0 ? _fields$model : this.model;\n    this.completionOptions = (_fields$completionOpt = fields === null || fields === void 0 ? void 0 : fields.completionOptions) !== null && _fields$completionOpt !== void 0 ? _fields$completionOpt : {};\n    this.globalContext = typeof window !== \"undefined\" ? window : globalThis;\n    this._ensureAiAvailable();\n  }\n  _llmType() {\n    return \"windowai\";\n  }\n  async _call(prompt, stopSequences) {\n    const input = typeof prompt === \"string\" ? {\n      prompt\n    } : {\n      messages: prompt\n    };\n    const options = {\n      ...this.completionOptions,\n      temperature: this.temperature,\n      maxTokens: this.maxTokens,\n      model: this.model,\n      //ModelID e.g. GPT3\n      stopSequences\n    };\n    try {\n      const output = await this.globalContext.ai.getCompletion(input, options);\n      return output.text;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not generate response from WindowAi.\");\n    }\n  }\n  async getCurrentModel() {\n    try {\n      const modelID = await this.globalContext.ai.getCurrentModel();\n      return modelID;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not retrieve current model from WindowAi.\");\n    }\n  }\n  async _ensureAiAvailable() {\n    let timeoutCounter = 0;\n    while (!this.globalContext.ai) {\n      await new Promise(resolve => setTimeout(resolve, 100));\n      timeoutCounter += 100;\n      if (timeoutCounter >= 1000) {\n        console.error(\"Please visit https://windowai.io to install WindowAi.\");\n        break;\n      }\n    }\n    if (this.globalContext.ai) {\n      console.log(\"WindowAi detected!\");\n    }\n  }\n}","map":{"version":3,"names":["LLM","ModelID","WindowAi","constructor","fields","_fields$temperature","_fields$maxTokens","_fields$model","_fields$completionOpt","temperature","maxTokens","model","completionOptions","globalContext","window","globalThis","_ensureAiAvailable","_llmType","_call","prompt","stopSequences","input","messages","options","output","ai","getCompletion","text","error","console","log","Error","getCurrentModel","modelID","timeoutCounter","Promise","resolve","setTimeout"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/src/WindowAi.ts"],"sourcesContent":["\nimport { LLM, BaseLLMParams } from \"langchain/llms/base.js\";\n\ninterface WindowAiInput extends BaseLLMParams {\n    temperature?: number;\n    maxTokens?: number;\n    model?: string;\n    completionOptions?: object;\n  }\n\n  interface CompletionOptions {\n\n    // If specified, partial updates will be streamed to this handler as they become available,\n    // and only the first partial update will be returned by the Promise.\n    onStreamResult?: (result: Output | null, error: string | null) => unknown\n  \n    // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n    // make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n    // Different models have different defaults.\n    temperature?: number\n  \n    // How many completion choices to generate. Defaults to 1.\n    numOutputs?: number\n  \n    // The maximum number of tokens to generate in the chat completion. Defaults to infinity, but the\n    // total length of input tokens and generated tokens is limited by the model's context length.\n    maxTokens?: number\n  \n    // Sequences where the API will stop generating further tokens.\n    stopSequences?: string[]\n  \n    // Identifier of the model to use. Defaults to the user's current model, but can be overridden here.\n    model?: ModelID\n  }\n  \n  export type ChatMessage = {\n    role: \"system\" | \"user\" | \"assistant\"\n    content: string\n  }\n  \nexport enum ModelID {\n  GPT3 = \"openai/gpt3.5\",\n  GPT4 = \"openai/gpt4\",\n  GPTNeo = \"together/gpt-neoxt-20B\",\n  Cohere = \"cohere/xlarge\",\n  Local = \"local\"\n}\nexport type Output = | { text: string } | { message: ChatMessage }\nexport type Input =  { prompt: string } | { messages: ChatMessage[] }\n\n\n// export declare class WindowAi extends LLM implements WindowAiInput {\n//   temperature: number;\n//   maxTokens: number;\n//   model: string;\n//   completionOptions: CompletionOptions;\n\n//   constructor(fields?: WindowAiInput);\n\n//   _llmType(): string;\n\n//   _call(prompt: string | ChatMessage[], _stop?: string[]): Promise<string>;\n\n//   getCurrentModel(): Promise<string>;\n\n//   _ensureAiAvailable(): Promise<void>;\n// }\n\n\n\n\n\n  export class WindowAi extends LLM implements WindowAiInput {\n    temperature = 0;\n    maxTokens = 250;\n    model: string;\n    completionOptions: object;\n    globalContext: any;\n  \n    constructor(fields?: WindowAiInput) {\n      super(fields ?? {});\n  \n      this.temperature = fields?.temperature ?? this.temperature;\n      this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n      this.model = fields?.model ?? this.model;\n      this.completionOptions = fields?.completionOptions ?? {};\n  \n      this.globalContext = (typeof window !== \"undefined\") ? window : globalThis;\n\n      \n  \n      this._ensureAiAvailable();\n    }\n  \n    _llmType(): string {\n      return \"windowai\";\n    }\n  \n    async _call(prompt: string, stopSequences?: string[]): Promise<string> {\n      const input = typeof prompt === \"string\" ? { prompt } : { messages: prompt };\n      const options = {\n        ...this.completionOptions,\n        temperature: this.temperature,\n        maxTokens: this.maxTokens,\n        model: this.model, //ModelID e.g. GPT3\n        stopSequences,\n      };\n  \n      try {\n        const output = await this.globalContext.ai.getCompletion(input, options);\n        return output.text;\n      } catch (error) {\n        console.log(error);\n        throw new Error(\"Could not generate response from WindowAi.\");\n      }\n    }\n  \n    async getCurrentModel(): Promise<string> {\n      try {\n        const modelID = await this.globalContext.ai.getCurrentModel();\n        return modelID;\n      } catch (error) {\n        console.log(error);\n        throw new Error(\"Could not retrieve current model from WindowAi.\");\n      }\n    }\n  \n    private async _ensureAiAvailable(): Promise<void> {\n      let timeoutCounter = 0;\n      while (!this.globalContext.ai) {\n        await new Promise((resolve) => setTimeout(resolve, 100));\n        timeoutCounter += 100;\n        if (timeoutCounter >= 1000) {\n          console.error(\"Please visit https://windowai.io to install WindowAi.\");\n          break;\n        }\n      }\n  \n      if (this.globalContext.ai) {\n        console.log(\"WindowAi detected!\");\n      }\n    }\n  }\n  "],"mappings":"AACA,SAASA,GAAG,QAAuB,wBAAwB;AAuC3D,WAAYC,OAAO,0BAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAA,OAAPA,OAAO;AAAA;AAWnB;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AAME,OAAO,MAAMC,QAAQ,SAASF,GAAG,CAA0B;EAOzDG,WAAWA,CAACC,MAAsB,EAAE;IAAA,IAAAC,mBAAA,EAAAC,iBAAA,EAAAC,aAAA,EAAAC,qBAAA;IAClC,KAAK,CAACJ,MAAM,aAANA,MAAM,cAANA,MAAM,GAAI,CAAC,CAAC,CAAC;IAAC,KAPtBK,WAAW,GAAG,CAAC;IAAA,KACfC,SAAS,GAAG,GAAG;IAAA,KACfC,KAAK;IAAA,KACLC,iBAAiB;IAAA,KACjBC,aAAa;IAKX,IAAI,CAACJ,WAAW,IAAAJ,mBAAA,GAAGD,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEK,WAAW,cAAAJ,mBAAA,cAAAA,mBAAA,GAAI,IAAI,CAACI,WAAW;IAC1D,IAAI,CAACC,SAAS,IAAAJ,iBAAA,GAAGF,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEM,SAAS,cAAAJ,iBAAA,cAAAA,iBAAA,GAAI,IAAI,CAACI,SAAS;IACpD,IAAI,CAACC,KAAK,IAAAJ,aAAA,GAAGH,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEO,KAAK,cAAAJ,aAAA,cAAAA,aAAA,GAAI,IAAI,CAACI,KAAK;IACxC,IAAI,CAACC,iBAAiB,IAAAJ,qBAAA,GAAGJ,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEQ,iBAAiB,cAAAJ,qBAAA,cAAAA,qBAAA,GAAI,CAAC,CAAC;IAExD,IAAI,CAACK,aAAa,GAAI,OAAOC,MAAM,KAAK,WAAW,GAAIA,MAAM,GAAGC,UAAU;IAI1E,IAAI,CAACC,kBAAkB,EAAE;EAC3B;EAEAC,QAAQA,CAAA,EAAW;IACjB,OAAO,UAAU;EACnB;EAEA,MAAMC,KAAKA,CAACC,MAAc,EAAEC,aAAwB,EAAmB;IACrE,MAAMC,KAAK,GAAG,OAAOF,MAAM,KAAK,QAAQ,GAAG;MAAEA;IAAO,CAAC,GAAG;MAAEG,QAAQ,EAAEH;IAAO,CAAC;IAC5E,MAAMI,OAAO,GAAG;MACd,GAAG,IAAI,CAACX,iBAAiB;MACzBH,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BC,SAAS,EAAE,IAAI,CAACA,SAAS;MACzBC,KAAK,EAAE,IAAI,CAACA,KAAK;MAAE;MACnBS;IACF,CAAC;IAED,IAAI;MACF,MAAMI,MAAM,GAAG,MAAM,IAAI,CAACX,aAAa,CAACY,EAAE,CAACC,aAAa,CAACL,KAAK,EAAEE,OAAO,CAAC;MACxE,OAAOC,MAAM,CAACG,IAAI;IACpB,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,4CAA4C,CAAC;IAC/D;EACF;EAEA,MAAMC,eAAeA,CAAA,EAAoB;IACvC,IAAI;MACF,MAAMC,OAAO,GAAG,MAAM,IAAI,CAACpB,aAAa,CAACY,EAAE,CAACO,eAAe,EAAE;MAC7D,OAAOC,OAAO;IAChB,CAAC,CAAC,OAAOL,KAAK,EAAE;MACdC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,iDAAiD,CAAC;IACpE;EACF;EAEA,MAAcf,kBAAkBA,CAAA,EAAkB;IAChD,IAAIkB,cAAc,GAAG,CAAC;IACtB,OAAO,CAAC,IAAI,CAACrB,aAAa,CAACY,EAAE,EAAE;MAC7B,MAAM,IAAIU,OAAO,CAAEC,OAAO,IAAKC,UAAU,CAACD,OAAO,EAAE,GAAG,CAAC,CAAC;MACxDF,cAAc,IAAI,GAAG;MACrB,IAAIA,cAAc,IAAI,IAAI,EAAE;QAC1BL,OAAO,CAACD,KAAK,CAAC,uDAAuD,CAAC;QACtE;MACF;IACF;IAEA,IAAI,IAAI,CAACf,aAAa,CAACY,EAAE,EAAE;MACzBI,OAAO,CAACC,GAAG,CAAC,oBAAoB,CAAC;IACnC;EACF;AACF"},"metadata":{},"sourceType":"module","externalDependencies":[]}
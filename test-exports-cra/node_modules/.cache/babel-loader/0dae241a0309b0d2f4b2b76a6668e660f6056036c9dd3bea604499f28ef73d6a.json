{"ast":null,"code":"import { AIChatMessage } from \"../schema/index.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { getBufferString } from \"../memory/base.js\";\nexport class BaseChatModel extends BaseLanguageModel {\n  constructor(_ref) {\n    let {\n      ...rest\n    } = _ref;\n    super(rest);\n  }\n  async generate(messages, stop) {\n    const generations = [];\n    const llmOutputs = [];\n    const messageStrings = messages.map(messageList => getBufferString(messageList));\n    await this.callbackManager.handleLLMStart({\n      name: this._llmType()\n    }, messageStrings, this.verbose);\n    try {\n      for (const message of messages) {\n        const result = await this._generate(message, stop);\n        if (result.llmOutput) {\n          llmOutputs.push(result.llmOutput);\n        }\n        generations.push(result.generations);\n      }\n    } catch (err) {\n      await this.callbackManager.handleLLMError(err, this.verbose);\n      throw err;\n    }\n    const output = {\n      generations,\n      llmOutput: llmOutputs.length ? this._combineLLMOutput?.(...llmOutputs) : undefined\n    };\n    await this.callbackManager.handleLLMEnd(output, this.verbose);\n    return output;\n  }\n  _modelType() {\n    return \"base_chat_model\";\n  }\n  async generatePrompt(promptValues, stop) {\n    const promptMessages = promptValues.map(promptValue => promptValue.toChatMessages());\n    return this.generate(promptMessages, stop);\n  }\n  async call(messages, stop) {\n    const result = await this.generate([messages], stop);\n    const generations = result.generations;\n    return generations[0][0].message;\n  }\n  async callPrompt(promptValue, stop) {\n    const promptMessages = promptValue.toChatMessages();\n    return this.call(promptMessages, stop);\n  }\n}\nexport class SimpleChatModel extends BaseChatModel {\n  async _generate(messages, stop) {\n    const text = await this._call(messages, stop);\n    const message = new AIChatMessage(text);\n    return {\n      generations: [{\n        text: message.text,\n        message\n      }]\n    };\n  }\n}","map":{"version":3,"names":["AIChatMessage","BaseLanguageModel","getBufferString","BaseChatModel","constructor","_ref","rest","generate","messages","stop","generations","llmOutputs","messageStrings","map","messageList","callbackManager","handleLLMStart","name","_llmType","verbose","message","result","_generate","llmOutput","push","err","handleLLMError","output","length","_combineLLMOutput","undefined","handleLLMEnd","_modelType","generatePrompt","promptValues","promptMessages","promptValue","toChatMessages","call","callPrompt","SimpleChatModel","text","_call"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/node_modules/langchain/dist/chat_models/base.js"],"sourcesContent":["import { AIChatMessage, } from \"../schema/index.js\";\nimport { BaseLanguageModel, } from \"../base_language/index.js\";\nimport { getBufferString } from \"../memory/base.js\";\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor({ ...rest }) {\n        super(rest);\n    }\n    async generate(messages, stop) {\n        const generations = [];\n        const llmOutputs = [];\n        const messageStrings = messages.map((messageList) => getBufferString(messageList));\n        await this.callbackManager.handleLLMStart({ name: this._llmType() }, messageStrings, this.verbose);\n        try {\n            for (const message of messages) {\n                const result = await this._generate(message, stop);\n                if (result.llmOutput) {\n                    llmOutputs.push(result.llmOutput);\n                }\n                generations.push(result.generations);\n            }\n        }\n        catch (err) {\n            await this.callbackManager.handleLLMError(err, this.verbose);\n            throw err;\n        }\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        await this.callbackManager.handleLLMEnd(output, this.verbose);\n        return output;\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    async generatePrompt(promptValues, stop) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, stop);\n    }\n    async call(messages, stop) {\n        const result = await this.generate([messages], stop);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    async callPrompt(promptValue, stop) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, stop);\n    }\n}\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, stop) {\n        const text = await this._call(messages, stop);\n        const message = new AIChatMessage(text);\n        return {\n            generations: [\n                {\n                    text: message.text,\n                    message,\n                },\n            ],\n        };\n    }\n}\n"],"mappings":"AAAA,SAASA,aAAa,QAAS,oBAAoB;AACnD,SAASC,iBAAiB,QAAS,2BAA2B;AAC9D,SAASC,eAAe,QAAQ,mBAAmB;AACnD,OAAO,MAAMC,aAAa,SAASF,iBAAiB,CAAC;EACjDG,WAAWA,CAAAC,IAAA,EAAc;IAAA,IAAb;MAAE,GAAGC;IAAK,CAAC,GAAAD,IAAA;IACnB,KAAK,CAACC,IAAI,CAAC;EACf;EACA,MAAMC,QAAQA,CAACC,QAAQ,EAAEC,IAAI,EAAE;IAC3B,MAAMC,WAAW,GAAG,EAAE;IACtB,MAAMC,UAAU,GAAG,EAAE;IACrB,MAAMC,cAAc,GAAGJ,QAAQ,CAACK,GAAG,CAAEC,WAAW,IAAKZ,eAAe,CAACY,WAAW,CAAC,CAAC;IAClF,MAAM,IAAI,CAACC,eAAe,CAACC,cAAc,CAAC;MAAEC,IAAI,EAAE,IAAI,CAACC,QAAQ;IAAG,CAAC,EAAEN,cAAc,EAAE,IAAI,CAACO,OAAO,CAAC;IAClG,IAAI;MACA,KAAK,MAAMC,OAAO,IAAIZ,QAAQ,EAAE;QAC5B,MAAMa,MAAM,GAAG,MAAM,IAAI,CAACC,SAAS,CAACF,OAAO,EAAEX,IAAI,CAAC;QAClD,IAAIY,MAAM,CAACE,SAAS,EAAE;UAClBZ,UAAU,CAACa,IAAI,CAACH,MAAM,CAACE,SAAS,CAAC;QACrC;QACAb,WAAW,CAACc,IAAI,CAACH,MAAM,CAACX,WAAW,CAAC;MACxC;IACJ,CAAC,CACD,OAAOe,GAAG,EAAE;MACR,MAAM,IAAI,CAACV,eAAe,CAACW,cAAc,CAACD,GAAG,EAAE,IAAI,CAACN,OAAO,CAAC;MAC5D,MAAMM,GAAG;IACb;IACA,MAAME,MAAM,GAAG;MACXjB,WAAW;MACXa,SAAS,EAAEZ,UAAU,CAACiB,MAAM,GACtB,IAAI,CAACC,iBAAiB,GAAG,GAAGlB,UAAU,CAAC,GACvCmB;IACV,CAAC;IACD,MAAM,IAAI,CAACf,eAAe,CAACgB,YAAY,CAACJ,MAAM,EAAE,IAAI,CAACR,OAAO,CAAC;IAC7D,OAAOQ,MAAM;EACjB;EACAK,UAAUA,CAAA,EAAG;IACT,OAAO,iBAAiB;EAC5B;EACA,MAAMC,cAAcA,CAACC,YAAY,EAAEzB,IAAI,EAAE;IACrC,MAAM0B,cAAc,GAAGD,YAAY,CAACrB,GAAG,CAAEuB,WAAW,IAAKA,WAAW,CAACC,cAAc,EAAE,CAAC;IACtF,OAAO,IAAI,CAAC9B,QAAQ,CAAC4B,cAAc,EAAE1B,IAAI,CAAC;EAC9C;EACA,MAAM6B,IAAIA,CAAC9B,QAAQ,EAAEC,IAAI,EAAE;IACvB,MAAMY,MAAM,GAAG,MAAM,IAAI,CAACd,QAAQ,CAAC,CAACC,QAAQ,CAAC,EAAEC,IAAI,CAAC;IACpD,MAAMC,WAAW,GAAGW,MAAM,CAACX,WAAW;IACtC,OAAOA,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACU,OAAO;EACpC;EACA,MAAMmB,UAAUA,CAACH,WAAW,EAAE3B,IAAI,EAAE;IAChC,MAAM0B,cAAc,GAAGC,WAAW,CAACC,cAAc,EAAE;IACnD,OAAO,IAAI,CAACC,IAAI,CAACH,cAAc,EAAE1B,IAAI,CAAC;EAC1C;AACJ;AACA,OAAO,MAAM+B,eAAe,SAASrC,aAAa,CAAC;EAC/C,MAAMmB,SAASA,CAACd,QAAQ,EAAEC,IAAI,EAAE;IAC5B,MAAMgC,IAAI,GAAG,MAAM,IAAI,CAACC,KAAK,CAAClC,QAAQ,EAAEC,IAAI,CAAC;IAC7C,MAAMW,OAAO,GAAG,IAAIpB,aAAa,CAACyC,IAAI,CAAC;IACvC,OAAO;MACH/B,WAAW,EAAE,CACT;QACI+B,IAAI,EAAErB,OAAO,CAACqB,IAAI;QAClBrB;MACJ,CAAC;IAET,CAAC;EACL;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}
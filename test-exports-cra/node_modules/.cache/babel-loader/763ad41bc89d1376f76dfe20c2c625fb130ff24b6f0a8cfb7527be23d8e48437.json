{"ast":null,"code":"import { Configuration, OpenAIApi } from \"openai\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { BaseLLM } from \"./base.js\";\nimport { calculateMaxTokens } from \"../base_language/count_tokens.js\";\nimport { OpenAIChat } from \"./openai-chat.js\";\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments OpenAIInput\n */\nexport class OpenAI extends BaseLLM {\n  constructor(fields, configuration) {\n    if (fields?.modelName?.startsWith(\"gpt-3.5-turbo\") || fields?.modelName?.startsWith(\"gpt-4\")) {\n      // eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\n      return new OpenAIChat(fields, configuration);\n    }\n    super(fields ?? {});\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0.7\n    });\n    Object.defineProperty(this, \"maxTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 256\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"frequencyPenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"presencePenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"n\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"bestOf\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"logitBias\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"text-davinci-003\"\n    });\n    Object.defineProperty(this, \"modelKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"batchSize\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 20\n    });\n    Object.defineProperty(this, \"timeout\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stop\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"clientConfig\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    const apiKey = fields?.openAIApiKey ?? (\n    // eslint-disable-next-line no-process-env\n    typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\n    if (!apiKey) {\n      throw new Error(\"OpenAI API key not found\");\n    }\n    this.modelName = fields?.modelName ?? this.modelName;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.batchSize = fields?.batchSize ?? this.batchSize;\n    this.timeout = fields?.timeout;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.n = fields?.n ?? this.n;\n    this.bestOf = fields?.bestOf ?? this.bestOf;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stop;\n    this.streaming = fields?.streaming ?? false;\n    if (this.streaming && this.n > 1) {\n      throw new Error(\"Cannot stream results when n > 1\");\n    }\n    if (this.streaming && this.bestOf > 1) {\n      throw new Error(\"Cannot stream results when bestOf > 1\");\n    }\n    this.clientConfig = {\n      apiKey,\n      ...configuration\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams() {\n    return {\n      model: this.modelName,\n      temperature: this.temperature,\n      max_tokens: this.maxTokens,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      n: this.n,\n      best_of: this.bestOf,\n      logit_bias: this.logitBias,\n      stop: this.stop,\n      stream: this.streaming,\n      ...this.modelKwargs\n    };\n  }\n  _identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams(),\n      ...this.clientConfig\n    };\n  }\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n  /**\n   * Call out to OpenAI's endpoint with k unique prompts\n   *\n   * @param prompts - The prompts to pass into the model.\n   * @param [stop] - Optional list of stop words to use when generating.\n   *\n   * @returns The full LLM output.\n   *\n   * @example\n   * ```ts\n   * import { OpenAI } from \"langchain/llms/openai\";\n   * const openai = new OpenAI();\n   * const response = await openai.generate([\"Tell me a joke.\"]);\n   * ```\n   */\n  async _generate(prompts, stop) {\n    const subPrompts = chunkArray(prompts, this.batchSize);\n    const choices = [];\n    const tokenUsage = {};\n    if (this.stop && stop) {\n      throw new Error(\"Stop found in input and default params\");\n    }\n    const params = this.invocationParams();\n    params.stop = stop ?? params.stop;\n    if (params.max_tokens === -1) {\n      if (prompts.length !== 1) {\n        throw new Error(\"max_tokens set to -1 not supported for multiple inputs\");\n      }\n      params.max_tokens = await calculateMaxTokens({\n        prompt: prompts[0],\n        // Cast here to allow for other models that may not fit the union\n        modelName: this.modelName\n      });\n    }\n    for (let i = 0; i < subPrompts.length; i += 1) {\n      const data = params.stream ? await new Promise((resolve, reject) => {\n        const choice = {};\n        let response;\n        let rejected = false;\n        this.completionWithRetry({\n          ...params,\n          prompt: subPrompts[i]\n        }, {\n          responseType: \"stream\",\n          onmessage: event => {\n            if (event.data?.trim?.() === \"[DONE]\") {\n              resolve({\n                ...response,\n                choices: [choice]\n              });\n            } else {\n              const message = JSON.parse(event.data);\n              // on the first message set the response properties\n              if (!response) {\n                response = {\n                  id: message.id,\n                  object: message.object,\n                  created: message.created,\n                  model: message.model\n                };\n              }\n              // on all messages, update choice\n              const part = message.choices[0];\n              if (part != null) {\n                choice.text = (choice.text ?? \"\") + (part.text ?? \"\");\n                choice.finish_reason = part.finish_reason;\n                choice.logprobs = part.logprobs;\n                // eslint-disable-next-line no-void\n                void this.callbackManager.handleLLMNewToken(part.text ?? \"\", true);\n              }\n            }\n          }\n        }).catch(error => {\n          if (!rejected) {\n            rejected = true;\n            reject(error);\n          }\n        });\n      }) : await this.completionWithRetry({\n        ...params,\n        prompt: subPrompts[i]\n      });\n      choices.push(...data.choices);\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens\n      } = data.usage ?? {};\n      if (completionTokens) {\n        tokenUsage.completionTokens = (tokenUsage.completionTokens ?? 0) + completionTokens;\n      }\n      if (promptTokens) {\n        tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n      }\n      if (totalTokens) {\n        tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n      }\n    }\n    const generations = chunkArray(choices, this.n).map(promptChoices => promptChoices.map(choice => ({\n      text: choice.text ?? \"\",\n      generationInfo: {\n        finishReason: choice.finish_reason,\n        logprobs: choice.logprobs\n      }\n    })));\n    return {\n      generations,\n      llmOutput: {\n        tokenUsage\n      }\n    };\n  }\n  /** @ignore */\n  async completionWithRetry(request, options) {\n    if (!this.client) {\n      const clientConfig = new Configuration({\n        ...this.clientConfig,\n        baseOptions: {\n          timeout: this.timeout,\n          adapter: fetchAdapter,\n          ...this.clientConfig.baseOptions\n        }\n      });\n      this.client = new OpenAIApi(clientConfig);\n    }\n    return this.caller.call(this.client.createCompletion.bind(this.client), request, options).then(res => res.data);\n  }\n  _llmType() {\n    return \"openai\";\n  }\n}\n/**\n * PromptLayer wrapper to OpenAI\n * @augments OpenAI\n */\nexport class PromptLayerOpenAI extends OpenAI {\n  constructor(fields) {\n    super(fields);\n    Object.defineProperty(this, \"promptLayerApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"plTags\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.plTags = fields?.plTags ?? [];\n    this.promptLayerApiKey = fields?.promptLayerApiKey ?? (typeof process !== \"undefined\" ?\n    // eslint-disable-next-line no-process-env\n    process.env.PROMPTLAYER_API_KEY : undefined);\n    if (!this.promptLayerApiKey) {\n      throw new Error(\"Missing PromptLayer API key\");\n    }\n  }\n  async completionWithRetry(request, options) {\n    if (request.stream) {\n      return super.completionWithRetry(request, options);\n    }\n    const requestStartTime = Date.now();\n    const response = await super.completionWithRetry(request);\n    const requestEndTime = Date.now();\n    // https://github.com/MagnivOrg/promptlayer-js-helper\n    await this.caller.call(fetch, \"https://api.promptlayer.com/track-request\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        Accept: \"application/json\"\n      },\n      body: JSON.stringify({\n        function_name: \"openai.Completion.create\",\n        args: [],\n        kwargs: {\n          engine: request.model,\n          prompt: request.prompt\n        },\n        tags: this.plTags ?? [],\n        request_response: response,\n        request_start_time: Math.floor(requestStartTime / 1000),\n        request_end_time: Math.floor(requestEndTime / 1000),\n        api_key: this.promptLayerApiKey\n      })\n    });\n    return response;\n  }\n}\nexport { OpenAIChat } from \"./openai-chat.js\";","map":{"version":3,"names":["Configuration","OpenAIApi","fetchAdapter","chunkArray","BaseLLM","calculateMaxTokens","OpenAIChat","OpenAI","constructor","fields","configuration","modelName","startsWith","Object","defineProperty","enumerable","configurable","writable","value","apiKey","openAIApiKey","process","env","OPENAI_API_KEY","undefined","Error","modelKwargs","batchSize","timeout","temperature","maxTokens","topP","frequencyPenalty","presencePenalty","n","bestOf","logitBias","stop","streaming","clientConfig","invocationParams","model","max_tokens","top_p","frequency_penalty","presence_penalty","best_of","logit_bias","stream","_identifyingParams","model_name","identifyingParams","_generate","prompts","subPrompts","choices","tokenUsage","params","length","prompt","i","data","Promise","resolve","reject","choice","response","rejected","completionWithRetry","responseType","onmessage","event","trim","message","JSON","parse","id","object","created","part","text","finish_reason","logprobs","callbackManager","handleLLMNewToken","catch","error","push","completion_tokens","completionTokens","prompt_tokens","promptTokens","total_tokens","totalTokens","usage","generations","map","promptChoices","generationInfo","finishReason","llmOutput","request","options","client","baseOptions","adapter","caller","call","createCompletion","bind","then","res","_llmType","PromptLayerOpenAI","plTags","promptLayerApiKey","PROMPTLAYER_API_KEY","requestStartTime","Date","now","requestEndTime","fetch","method","headers","Accept","body","stringify","function_name","args","kwargs","engine","tags","request_response","request_start_time","Math","floor","request_end_time","api_key"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/node_modules/langchain/dist/llms/openai.js"],"sourcesContent":["import { Configuration, OpenAIApi, } from \"openai\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { BaseLLM } from \"./base.js\";\nimport { calculateMaxTokens } from \"../base_language/count_tokens.js\";\nimport { OpenAIChat } from \"./openai-chat.js\";\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments OpenAIInput\n */\nexport class OpenAI extends BaseLLM {\n    constructor(fields, configuration) {\n        if (fields?.modelName?.startsWith(\"gpt-3.5-turbo\") ||\n            fields?.modelName?.startsWith(\"gpt-4\")) {\n            // eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\n            return new OpenAIChat(fields, configuration);\n        }\n        super(fields ?? {});\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0.7\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 256\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"bestOf\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text-davinci-003\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"batchSize\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 20\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const apiKey = fields?.openAIApiKey ??\n            // eslint-disable-next-line no-process-env\n            (typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\n        if (!apiKey) {\n            throw new Error(\"OpenAI API key not found\");\n        }\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.batchSize = fields?.batchSize ?? this.batchSize;\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.n = fields?.n ?? this.n;\n        this.bestOf = fields?.bestOf ?? this.bestOf;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.streaming = fields?.streaming ?? false;\n        if (this.streaming && this.n > 1) {\n            throw new Error(\"Cannot stream results when n > 1\");\n        }\n        if (this.streaming && this.bestOf > 1) {\n            throw new Error(\"Cannot stream results when bestOf > 1\");\n        }\n        this.clientConfig = {\n            apiKey,\n            ...configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams() {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            max_tokens: this.maxTokens,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            n: this.n,\n            best_of: this.bestOf,\n            logit_bias: this.logitBias,\n            stop: this.stop,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /**\n     * Call out to OpenAI's endpoint with k unique prompts\n     *\n     * @param prompts - The prompts to pass into the model.\n     * @param [stop] - Optional list of stop words to use when generating.\n     *\n     * @returns The full LLM output.\n     *\n     * @example\n     * ```ts\n     * import { OpenAI } from \"langchain/llms/openai\";\n     * const openai = new OpenAI();\n     * const response = await openai.generate([\"Tell me a joke.\"]);\n     * ```\n     */\n    async _generate(prompts, stop) {\n        const subPrompts = chunkArray(prompts, this.batchSize);\n        const choices = [];\n        const tokenUsage = {};\n        if (this.stop && stop) {\n            throw new Error(\"Stop found in input and default params\");\n        }\n        const params = this.invocationParams();\n        params.stop = stop ?? params.stop;\n        if (params.max_tokens === -1) {\n            if (prompts.length !== 1) {\n                throw new Error(\"max_tokens set to -1 not supported for multiple inputs\");\n            }\n            params.max_tokens = await calculateMaxTokens({\n                prompt: prompts[0],\n                // Cast here to allow for other models that may not fit the union\n                modelName: this.modelName,\n            });\n        }\n        for (let i = 0; i < subPrompts.length; i += 1) {\n            const data = params.stream\n                ? await new Promise((resolve, reject) => {\n                    const choice = {};\n                    let response;\n                    let rejected = false;\n                    this.completionWithRetry({\n                        ...params,\n                        prompt: subPrompts[i],\n                    }, {\n                        responseType: \"stream\",\n                        onmessage: (event) => {\n                            if (event.data?.trim?.() === \"[DONE]\") {\n                                resolve({\n                                    ...response,\n                                    choices: [choice],\n                                });\n                            }\n                            else {\n                                const message = JSON.parse(event.data);\n                                // on the first message set the response properties\n                                if (!response) {\n                                    response = {\n                                        id: message.id,\n                                        object: message.object,\n                                        created: message.created,\n                                        model: message.model,\n                                    };\n                                }\n                                // on all messages, update choice\n                                const part = message.choices[0];\n                                if (part != null) {\n                                    choice.text = (choice.text ?? \"\") + (part.text ?? \"\");\n                                    choice.finish_reason = part.finish_reason;\n                                    choice.logprobs = part.logprobs;\n                                    // eslint-disable-next-line no-void\n                                    void this.callbackManager.handleLLMNewToken(part.text ?? \"\", true);\n                                }\n                            }\n                        },\n                    }).catch((error) => {\n                        if (!rejected) {\n                            rejected = true;\n                            reject(error);\n                        }\n                    });\n                })\n                : await this.completionWithRetry({\n                    ...params,\n                    prompt: subPrompts[i],\n                });\n            choices.push(...data.choices);\n            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage ?? {};\n            if (completionTokens) {\n                tokenUsage.completionTokens =\n                    (tokenUsage.completionTokens ?? 0) + completionTokens;\n            }\n            if (promptTokens) {\n                tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n            }\n            if (totalTokens) {\n                tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n            }\n        }\n        const generations = chunkArray(choices, this.n).map((promptChoices) => promptChoices.map((choice) => ({\n            text: choice.text ?? \"\",\n            generationInfo: {\n                finishReason: choice.finish_reason,\n                logprobs: choice.logprobs,\n            },\n        })));\n        return {\n            generations,\n            llmOutput: { tokenUsage },\n        };\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const clientConfig = new Configuration({\n                ...this.clientConfig,\n                baseOptions: {\n                    timeout: this.timeout,\n                    adapter: fetchAdapter,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new OpenAIApi(clientConfig);\n        }\n        return this.caller\n            .call(this.client.createCompletion.bind(this.client), request, options)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n}\n/**\n * PromptLayer wrapper to OpenAI\n * @augments OpenAI\n */\nexport class PromptLayerOpenAI extends OpenAI {\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"promptLayerApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"plTags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.plTags = fields?.plTags ?? [];\n        this.promptLayerApiKey =\n            fields?.promptLayerApiKey ??\n                (typeof process !== \"undefined\"\n                    ? // eslint-disable-next-line no-process-env\n                        process.env.PROMPTLAYER_API_KEY\n                    : undefined);\n        if (!this.promptLayerApiKey) {\n            throw new Error(\"Missing PromptLayer API key\");\n        }\n    }\n    async completionWithRetry(request, options) {\n        if (request.stream) {\n            return super.completionWithRetry(request, options);\n        }\n        const requestStartTime = Date.now();\n        const response = await super.completionWithRetry(request);\n        const requestEndTime = Date.now();\n        // https://github.com/MagnivOrg/promptlayer-js-helper\n        await this.caller.call(fetch, \"https://api.promptlayer.com/track-request\", {\n            method: \"POST\",\n            headers: {\n                \"Content-Type\": \"application/json\",\n                Accept: \"application/json\",\n            },\n            body: JSON.stringify({\n                function_name: \"openai.Completion.create\",\n                args: [],\n                kwargs: { engine: request.model, prompt: request.prompt },\n                tags: this.plTags ?? [],\n                request_response: response,\n                request_start_time: Math.floor(requestStartTime / 1000),\n                request_end_time: Math.floor(requestEndTime / 1000),\n                api_key: this.promptLayerApiKey,\n            }),\n        });\n        return response;\n    }\n}\nexport { OpenAIChat } from \"./openai-chat.js\";\n"],"mappings":"AAAA,SAASA,aAAa,EAAEC,SAAS,QAAS,QAAQ;AAClD,OAAOC,YAAY,MAAM,gCAAgC;AACzD,SAASC,UAAU,QAAQ,kBAAkB;AAC7C,SAASC,OAAO,QAAQ,WAAW;AACnC,SAASC,kBAAkB,QAAQ,kCAAkC;AACrE,SAASC,UAAU,QAAQ,kBAAkB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,MAAM,SAASH,OAAO,CAAC;EAChCI,WAAWA,CAACC,MAAM,EAAEC,aAAa,EAAE;IAC/B,IAAID,MAAM,EAAEE,SAAS,EAAEC,UAAU,CAAC,eAAe,CAAC,IAC9CH,MAAM,EAAEE,SAAS,EAAEC,UAAU,CAAC,OAAO,CAAC,EAAE;MACxC;MACA,OAAO,IAAIN,UAAU,CAACG,MAAM,EAAEC,aAAa,CAAC;IAChD;IACA,KAAK,CAACD,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBI,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,GAAG,EAAE;MAC7BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,MAAMC,MAAM,GAAGV,MAAM,EAAEW,YAAY;IAC/B;IACC,OAAOC,OAAO,KAAK,WAAW,GAAGA,OAAO,CAACC,GAAG,CAACC,cAAc,GAAGC,SAAS,CAAC;IAC7E,IAAI,CAACL,MAAM,EAAE;MACT,MAAM,IAAIM,KAAK,CAAC,0BAA0B,CAAC;IAC/C;IACA,IAAI,CAACd,SAAS,GAAGF,MAAM,EAAEE,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACe,WAAW,GAAGjB,MAAM,EAAEiB,WAAW,IAAI,CAAC,CAAC;IAC5C,IAAI,CAACC,SAAS,GAAGlB,MAAM,EAAEkB,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,OAAO,GAAGnB,MAAM,EAAEmB,OAAO;IAC9B,IAAI,CAACC,WAAW,GAAGpB,MAAM,EAAEoB,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,SAAS,GAAGrB,MAAM,EAAEqB,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,IAAI,GAAGtB,MAAM,EAAEsB,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,gBAAgB,GAAGvB,MAAM,EAAEuB,gBAAgB,IAAI,IAAI,CAACA,gBAAgB;IACzE,IAAI,CAACC,eAAe,GAAGxB,MAAM,EAAEwB,eAAe,IAAI,IAAI,CAACA,eAAe;IACtE,IAAI,CAACC,CAAC,GAAGzB,MAAM,EAAEyB,CAAC,IAAI,IAAI,CAACA,CAAC;IAC5B,IAAI,CAACC,MAAM,GAAG1B,MAAM,EAAE0B,MAAM,IAAI,IAAI,CAACA,MAAM;IAC3C,IAAI,CAACC,SAAS,GAAG3B,MAAM,EAAE2B,SAAS;IAClC,IAAI,CAACC,IAAI,GAAG5B,MAAM,EAAE4B,IAAI;IACxB,IAAI,CAACC,SAAS,GAAG7B,MAAM,EAAE6B,SAAS,IAAI,KAAK;IAC3C,IAAI,IAAI,CAACA,SAAS,IAAI,IAAI,CAACJ,CAAC,GAAG,CAAC,EAAE;MAC9B,MAAM,IAAIT,KAAK,CAAC,kCAAkC,CAAC;IACvD;IACA,IAAI,IAAI,CAACa,SAAS,IAAI,IAAI,CAACH,MAAM,GAAG,CAAC,EAAE;MACnC,MAAM,IAAIV,KAAK,CAAC,uCAAuC,CAAC;IAC5D;IACA,IAAI,CAACc,YAAY,GAAG;MAChBpB,MAAM;MACN,GAAGT;IACP,CAAC;EACL;EACA;AACJ;AACA;EACI8B,gBAAgBA,CAAA,EAAG;IACf,OAAO;MACHC,KAAK,EAAE,IAAI,CAAC9B,SAAS;MACrBkB,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7Ba,UAAU,EAAE,IAAI,CAACZ,SAAS;MAC1Ba,KAAK,EAAE,IAAI,CAACZ,IAAI;MAChBa,iBAAiB,EAAE,IAAI,CAACZ,gBAAgB;MACxCa,gBAAgB,EAAE,IAAI,CAACZ,eAAe;MACtCC,CAAC,EAAE,IAAI,CAACA,CAAC;MACTY,OAAO,EAAE,IAAI,CAACX,MAAM;MACpBY,UAAU,EAAE,IAAI,CAACX,SAAS;MAC1BC,IAAI,EAAE,IAAI,CAACA,IAAI;MACfW,MAAM,EAAE,IAAI,CAACV,SAAS;MACtB,GAAG,IAAI,CAACZ;IACZ,CAAC;EACL;EACAuB,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHC,UAAU,EAAE,IAAI,CAACvC,SAAS;MAC1B,GAAG,IAAI,CAAC6B,gBAAgB,EAAE;MAC1B,GAAG,IAAI,CAACD;IACZ,CAAC;EACL;EACA;AACJ;AACA;EACIY,iBAAiBA,CAAA,EAAG;IAChB,OAAO,IAAI,CAACF,kBAAkB,EAAE;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMG,SAASA,CAACC,OAAO,EAAEhB,IAAI,EAAE;IAC3B,MAAMiB,UAAU,GAAGnD,UAAU,CAACkD,OAAO,EAAE,IAAI,CAAC1B,SAAS,CAAC;IACtD,MAAM4B,OAAO,GAAG,EAAE;IAClB,MAAMC,UAAU,GAAG,CAAC,CAAC;IACrB,IAAI,IAAI,CAACnB,IAAI,IAAIA,IAAI,EAAE;MACnB,MAAM,IAAIZ,KAAK,CAAC,wCAAwC,CAAC;IAC7D;IACA,MAAMgC,MAAM,GAAG,IAAI,CAACjB,gBAAgB,EAAE;IACtCiB,MAAM,CAACpB,IAAI,GAAGA,IAAI,IAAIoB,MAAM,CAACpB,IAAI;IACjC,IAAIoB,MAAM,CAACf,UAAU,KAAK,CAAC,CAAC,EAAE;MAC1B,IAAIW,OAAO,CAACK,MAAM,KAAK,CAAC,EAAE;QACtB,MAAM,IAAIjC,KAAK,CAAC,wDAAwD,CAAC;MAC7E;MACAgC,MAAM,CAACf,UAAU,GAAG,MAAMrC,kBAAkB,CAAC;QACzCsD,MAAM,EAAEN,OAAO,CAAC,CAAC,CAAC;QAClB;QACA1C,SAAS,EAAE,IAAI,CAACA;MACpB,CAAC,CAAC;IACN;IACA,KAAK,IAAIiD,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,UAAU,CAACI,MAAM,EAAEE,CAAC,IAAI,CAAC,EAAE;MAC3C,MAAMC,IAAI,GAAGJ,MAAM,CAACT,MAAM,GACpB,MAAM,IAAIc,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;QACrC,MAAMC,MAAM,GAAG,CAAC,CAAC;QACjB,IAAIC,QAAQ;QACZ,IAAIC,QAAQ,GAAG,KAAK;QACpB,IAAI,CAACC,mBAAmB,CAAC;UACrB,GAAGX,MAAM;UACTE,MAAM,EAAEL,UAAU,CAACM,CAAC;QACxB,CAAC,EAAE;UACCS,YAAY,EAAE,QAAQ;UACtBC,SAAS,EAAGC,KAAK,IAAK;YAClB,IAAIA,KAAK,CAACV,IAAI,EAAEW,IAAI,IAAI,KAAK,QAAQ,EAAE;cACnCT,OAAO,CAAC;gBACJ,GAAGG,QAAQ;gBACXX,OAAO,EAAE,CAACU,MAAM;cACpB,CAAC,CAAC;YACN,CAAC,MACI;cACD,MAAMQ,OAAO,GAAGC,IAAI,CAACC,KAAK,CAACJ,KAAK,CAACV,IAAI,CAAC;cACtC;cACA,IAAI,CAACK,QAAQ,EAAE;gBACXA,QAAQ,GAAG;kBACPU,EAAE,EAAEH,OAAO,CAACG,EAAE;kBACdC,MAAM,EAAEJ,OAAO,CAACI,MAAM;kBACtBC,OAAO,EAAEL,OAAO,CAACK,OAAO;kBACxBrC,KAAK,EAAEgC,OAAO,CAAChC;gBACnB,CAAC;cACL;cACA;cACA,MAAMsC,IAAI,GAAGN,OAAO,CAAClB,OAAO,CAAC,CAAC,CAAC;cAC/B,IAAIwB,IAAI,IAAI,IAAI,EAAE;gBACdd,MAAM,CAACe,IAAI,GAAG,CAACf,MAAM,CAACe,IAAI,IAAI,EAAE,KAAKD,IAAI,CAACC,IAAI,IAAI,EAAE,CAAC;gBACrDf,MAAM,CAACgB,aAAa,GAAGF,IAAI,CAACE,aAAa;gBACzChB,MAAM,CAACiB,QAAQ,GAAGH,IAAI,CAACG,QAAQ;gBAC/B;gBACA,KAAK,IAAI,CAACC,eAAe,CAACC,iBAAiB,CAACL,IAAI,CAACC,IAAI,IAAI,EAAE,EAAE,IAAI,CAAC;cACtE;YACJ;UACJ;QACJ,CAAC,CAAC,CAACK,KAAK,CAAEC,KAAK,IAAK;UAChB,IAAI,CAACnB,QAAQ,EAAE;YACXA,QAAQ,GAAG,IAAI;YACfH,MAAM,CAACsB,KAAK,CAAC;UACjB;QACJ,CAAC,CAAC;MACN,CAAC,CAAC,GACA,MAAM,IAAI,CAAClB,mBAAmB,CAAC;QAC7B,GAAGX,MAAM;QACTE,MAAM,EAAEL,UAAU,CAACM,CAAC;MACxB,CAAC,CAAC;MACNL,OAAO,CAACgC,IAAI,CAAC,GAAG1B,IAAI,CAACN,OAAO,CAAC;MAC7B,MAAM;QAAEiC,iBAAiB,EAAEC,gBAAgB;QAAEC,aAAa,EAAEC,YAAY;QAAEC,YAAY,EAAEC;MAAa,CAAC,GAAGhC,IAAI,CAACiC,KAAK,IAAI,CAAC,CAAC;MACzH,IAAIL,gBAAgB,EAAE;QAClBjC,UAAU,CAACiC,gBAAgB,GACvB,CAACjC,UAAU,CAACiC,gBAAgB,IAAI,CAAC,IAAIA,gBAAgB;MAC7D;MACA,IAAIE,YAAY,EAAE;QACdnC,UAAU,CAACmC,YAAY,GAAG,CAACnC,UAAU,CAACmC,YAAY,IAAI,CAAC,IAAIA,YAAY;MAC3E;MACA,IAAIE,WAAW,EAAE;QACbrC,UAAU,CAACqC,WAAW,GAAG,CAACrC,UAAU,CAACqC,WAAW,IAAI,CAAC,IAAIA,WAAW;MACxE;IACJ;IACA,MAAME,WAAW,GAAG5F,UAAU,CAACoD,OAAO,EAAE,IAAI,CAACrB,CAAC,CAAC,CAAC8D,GAAG,CAAEC,aAAa,IAAKA,aAAa,CAACD,GAAG,CAAE/B,MAAM,KAAM;MAClGe,IAAI,EAAEf,MAAM,CAACe,IAAI,IAAI,EAAE;MACvBkB,cAAc,EAAE;QACZC,YAAY,EAAElC,MAAM,CAACgB,aAAa;QAClCC,QAAQ,EAAEjB,MAAM,CAACiB;MACrB;IACJ,CAAC,CAAC,CAAC,CAAC;IACJ,OAAO;MACHa,WAAW;MACXK,SAAS,EAAE;QAAE5C;MAAW;IAC5B,CAAC;EACL;EACA;EACA,MAAMY,mBAAmBA,CAACiC,OAAO,EAAEC,OAAO,EAAE;IACxC,IAAI,CAAC,IAAI,CAACC,MAAM,EAAE;MACd,MAAMhE,YAAY,GAAG,IAAIvC,aAAa,CAAC;QACnC,GAAG,IAAI,CAACuC,YAAY;QACpBiE,WAAW,EAAE;UACT5E,OAAO,EAAE,IAAI,CAACA,OAAO;UACrB6E,OAAO,EAAEvG,YAAY;UACrB,GAAG,IAAI,CAACqC,YAAY,CAACiE;QACzB;MACJ,CAAC,CAAC;MACF,IAAI,CAACD,MAAM,GAAG,IAAItG,SAAS,CAACsC,YAAY,CAAC;IAC7C;IACA,OAAO,IAAI,CAACmE,MAAM,CACbC,IAAI,CAAC,IAAI,CAACJ,MAAM,CAACK,gBAAgB,CAACC,IAAI,CAAC,IAAI,CAACN,MAAM,CAAC,EAAEF,OAAO,EAAEC,OAAO,CAAC,CACtEQ,IAAI,CAAEC,GAAG,IAAKA,GAAG,CAAClD,IAAI,CAAC;EAChC;EACAmD,QAAQA,CAAA,EAAG;IACP,OAAO,QAAQ;EACnB;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,iBAAiB,SAAS1G,MAAM,CAAC;EAC1CC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACbI,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACgG,MAAM,GAAGzG,MAAM,EAAEyG,MAAM,IAAI,EAAE;IAClC,IAAI,CAACC,iBAAiB,GAClB1G,MAAM,EAAE0G,iBAAiB,KACpB,OAAO9F,OAAO,KAAK,WAAW;IACzB;IACEA,OAAO,CAACC,GAAG,CAAC8F,mBAAmB,GACjC5F,SAAS,CAAC;IACxB,IAAI,CAAC,IAAI,CAAC2F,iBAAiB,EAAE;MACzB,MAAM,IAAI1F,KAAK,CAAC,6BAA6B,CAAC;IAClD;EACJ;EACA,MAAM2C,mBAAmBA,CAACiC,OAAO,EAAEC,OAAO,EAAE;IACxC,IAAID,OAAO,CAACrD,MAAM,EAAE;MAChB,OAAO,KAAK,CAACoB,mBAAmB,CAACiC,OAAO,EAAEC,OAAO,CAAC;IACtD;IACA,MAAMe,gBAAgB,GAAGC,IAAI,CAACC,GAAG,EAAE;IACnC,MAAMrD,QAAQ,GAAG,MAAM,KAAK,CAACE,mBAAmB,CAACiC,OAAO,CAAC;IACzD,MAAMmB,cAAc,GAAGF,IAAI,CAACC,GAAG,EAAE;IACjC;IACA,MAAM,IAAI,CAACb,MAAM,CAACC,IAAI,CAACc,KAAK,EAAE,2CAA2C,EAAE;MACvEC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACL,cAAc,EAAE,kBAAkB;QAClCC,MAAM,EAAE;MACZ,CAAC;MACDC,IAAI,EAAEnD,IAAI,CAACoD,SAAS,CAAC;QACjBC,aAAa,EAAE,0BAA0B;QACzCC,IAAI,EAAE,EAAE;QACRC,MAAM,EAAE;UAAEC,MAAM,EAAE7B,OAAO,CAAC5D,KAAK;UAAEkB,MAAM,EAAE0C,OAAO,CAAC1C;QAAO,CAAC;QACzDwE,IAAI,EAAE,IAAI,CAACjB,MAAM,IAAI,EAAE;QACvBkB,gBAAgB,EAAElE,QAAQ;QAC1BmE,kBAAkB,EAAEC,IAAI,CAACC,KAAK,CAAClB,gBAAgB,GAAG,IAAI,CAAC;QACvDmB,gBAAgB,EAAEF,IAAI,CAACC,KAAK,CAACf,cAAc,GAAG,IAAI,CAAC;QACnDiB,OAAO,EAAE,IAAI,CAACtB;MAClB,CAAC;IACL,CAAC,CAAC;IACF,OAAOjD,QAAQ;EACnB;AACJ;AACA,SAAS5D,UAAU,QAAQ,kBAAkB"},"metadata":{},"sourceType":"module","externalDependencies":[]}
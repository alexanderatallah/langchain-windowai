{"ast":null,"code":"import { LLM } from \"langchain/llms/base.js\";\nexport let ModelID = /*#__PURE__*/function (ModelID) {\n  ModelID[\"GPT3\"] = \"openai/gpt3.5\";\n  ModelID[\"GPT4\"] = \"openai/gpt4\";\n  ModelID[\"GPTNeo\"] = \"together/gpt-neoxt-20B\";\n  ModelID[\"Cohere\"] = \"cohere/xlarge\";\n  ModelID[\"Local\"] = \"local\";\n  return ModelID;\n}({});\nexport class WindowAi extends LLM {\n  constructor(fields) {\n    var _fields$completionOpt;\n    super(fields !== null && fields !== void 0 ? fields : {});\n    this.completionOptions = void 0;\n    this.globalContext = void 0;\n    this.completionOptions = (_fields$completionOpt = fields === null || fields === void 0 ? void 0 : fields.completionOptions) !== null && _fields$completionOpt !== void 0 ? _fields$completionOpt : {};\n    this.globalContext = typeof window !== \"undefined\" ? window : globalThis;\n    this._ensureAiAvailable();\n  }\n  _llmType() {\n    return \"windowai\";\n  }\n  async _call(prompt) {\n    await this._ensureAiAvailable();\n    const input = typeof prompt === \"string\" ? {\n      prompt\n    } : {\n      messages: prompt\n    };\n    try {\n      const output = await this.globalContext.ai.getCompletion(input, this.completionOptions);\n      return output.text;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not generate response from WindowAi.\");\n    }\n  }\n  async getCurrentModel() {\n    try {\n      const modelID = await this.globalContext.ai.getCurrentModel();\n      return modelID;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not retrieve current model from WindowAi.\");\n    }\n  }\n  async _ensureAiAvailable() {\n    let timeoutCounter = 0;\n    while (!this.globalContext.ai) {\n      await new Promise(resolve => setTimeout(resolve, 100));\n      timeoutCounter += 100;\n      if (timeoutCounter >= 1000) {\n        console.error(\"Please visit https://windowai.io to install WindowAi.\");\n        break;\n      }\n    }\n    if (this.globalContext.ai) {\n      console.log(\"WindowAi detected!\");\n    }\n  }\n}","map":{"version":3,"names":["LLM","ModelID","WindowAi","constructor","fields","_fields$completionOpt","completionOptions","globalContext","window","globalThis","_ensureAiAvailable","_llmType","_call","prompt","input","messages","output","ai","getCompletion","text","error","console","log","Error","getCurrentModel","modelID","timeoutCounter","Promise","resolve","setTimeout"],"sources":["/Users/b/Code/langchainjs/test-exports-cra/src/WindowAi.ts"],"sourcesContent":["import { LLM, BaseLLMParams } from \"langchain/llms/base.js\";\n\nexport enum ModelID {\n  GPT3 = \"openai/gpt3.5\",\n  GPT4 = \"openai/gpt4\",\n  GPTNeo = \"together/gpt-neoxt-20B\",\n  Cohere = \"cohere/xlarge\",\n  Local = \"local\",\n}\n\nexport interface CompletionOptions {\n  onStreamResult?: (result: Output | null, error: string | null) => unknown;\n  temperature?: number;\n  numOutputs?: number;\n  maxTokens?: number;\n  stopSequences?: string[];\n  model?: ModelID;\n}\n\nexport type ChatMessage = {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n};\n\nexport type Output = { text: string } | { message: ChatMessage };\nexport type Input = { prompt: string } | { messages: ChatMessage[] };\n\ninterface WindowAiInput extends BaseLLMParams {\n  completionOptions?: CompletionOptions;\n}\n\nexport class WindowAi extends LLM implements WindowAiInput {\n  completionOptions: CompletionOptions;\n  globalContext: any;\n\n  constructor(fields?: WindowAiInput) {\n    super(fields ?? {});\n\n    this.completionOptions = fields?.completionOptions ?? {};\n\n    this.globalContext =\n      typeof window !== \"undefined\" ? window : globalThis;\n\n    this._ensureAiAvailable();\n  }\n\n  _llmType(): string {\n    return \"windowai\";\n  }\n\n  async _call(prompt: string): Promise<string> {\n    await this._ensureAiAvailable()\n    const input: Input = typeof prompt === \"string\" ? { prompt } : { messages: prompt };\n\n    try {\n      const output = await this.globalContext.ai.getCompletion(input, this.completionOptions);\n      return output.text;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not generate response from WindowAi.\");\n    }\n  }\n\n  async getCurrentModel(): Promise<string> {\n    try {\n      const modelID = await this.globalContext.ai.getCurrentModel();\n      return modelID;\n    } catch (error) {\n      console.log(error);\n      throw new Error(\"Could not retrieve current model from WindowAi.\");\n    }\n  }\n\n  private async _ensureAiAvailable(): Promise<void> {\n    let timeoutCounter = 0;\n    while (!this.globalContext.ai) {\n      await new Promise((resolve) => setTimeout(resolve, 100));\n      timeoutCounter += 100;\n      if (timeoutCounter >= 1000) {\n        console.error(\"Please visit https://windowai.io to install WindowAi.\");\n        break;\n      }\n    }\n\n    if (this.globalContext.ai) {\n      console.log(\"WindowAi detected!\");\n    }\n  }\n}\n"],"mappings":"AAAA,SAASA,GAAG,QAAuB,wBAAwB;AAE3D,WAAYC,OAAO,0BAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAPA,OAAO;EAAA,OAAPA,OAAO;AAAA;AA6BnB,OAAO,MAAMC,QAAQ,SAASF,GAAG,CAA0B;EAIzDG,WAAWA,CAACC,MAAsB,EAAE;IAAA,IAAAC,qBAAA;IAClC,KAAK,CAACD,MAAM,aAANA,MAAM,cAANA,MAAM,GAAI,CAAC,CAAC,CAAC;IAAC,KAJtBE,iBAAiB;IAAA,KACjBC,aAAa;IAKX,IAAI,CAACD,iBAAiB,IAAAD,qBAAA,GAAGD,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEE,iBAAiB,cAAAD,qBAAA,cAAAA,qBAAA,GAAI,CAAC,CAAC;IAExD,IAAI,CAACE,aAAa,GAChB,OAAOC,MAAM,KAAK,WAAW,GAAGA,MAAM,GAAGC,UAAU;IAErD,IAAI,CAACC,kBAAkB,EAAE;EAC3B;EAEAC,QAAQA,CAAA,EAAW;IACjB,OAAO,UAAU;EACnB;EAEA,MAAMC,KAAKA,CAACC,MAAc,EAAmB;IAC3C,MAAM,IAAI,CAACH,kBAAkB,EAAE;IAC/B,MAAMI,KAAY,GAAG,OAAOD,MAAM,KAAK,QAAQ,GAAG;MAAEA;IAAO,CAAC,GAAG;MAAEE,QAAQ,EAAEF;IAAO,CAAC;IAEnF,IAAI;MACF,MAAMG,MAAM,GAAG,MAAM,IAAI,CAACT,aAAa,CAACU,EAAE,CAACC,aAAa,CAACJ,KAAK,EAAE,IAAI,CAACR,iBAAiB,CAAC;MACvF,OAAOU,MAAM,CAACG,IAAI;IACpB,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,4CAA4C,CAAC;IAC/D;EACF;EAEA,MAAMC,eAAeA,CAAA,EAAoB;IACvC,IAAI;MACF,MAAMC,OAAO,GAAG,MAAM,IAAI,CAAClB,aAAa,CAACU,EAAE,CAACO,eAAe,EAAE;MAC7D,OAAOC,OAAO;IAChB,CAAC,CAAC,OAAOL,KAAK,EAAE;MACdC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;MAClB,MAAM,IAAIG,KAAK,CAAC,iDAAiD,CAAC;IACpE;EACF;EAEA,MAAcb,kBAAkBA,CAAA,EAAkB;IAChD,IAAIgB,cAAc,GAAG,CAAC;IACtB,OAAO,CAAC,IAAI,CAACnB,aAAa,CAACU,EAAE,EAAE;MAC7B,MAAM,IAAIU,OAAO,CAAEC,OAAO,IAAKC,UAAU,CAACD,OAAO,EAAE,GAAG,CAAC,CAAC;MACxDF,cAAc,IAAI,GAAG;MACrB,IAAIA,cAAc,IAAI,IAAI,EAAE;QAC1BL,OAAO,CAACD,KAAK,CAAC,uDAAuD,CAAC;QACtE;MACF;IACF;IAEA,IAAI,IAAI,CAACb,aAAa,CAACU,EAAE,EAAE;MACzBI,OAAO,CAACC,GAAG,CAAC,oBAAoB,CAAC;IACnC;EACF;AACF"},"metadata":{},"sourceType":"module","externalDependencies":[]}